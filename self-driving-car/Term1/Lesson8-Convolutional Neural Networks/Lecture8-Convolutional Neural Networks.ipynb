{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro To CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD//2Q==\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/B61jxZ4rkMs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x103477668>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('B61jxZ4rkMs', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAP/9k=\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/BdQccpMwk80\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ff780>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('BdQccpMwk80', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUIZ QUESTION\n",
    "\n",
    "What would be easier for your classifier to learn?\n",
    "- (A) R, G, B\n",
    "- (B) (R + G + B) / 3\n",
    "\n",
    "Answer: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/xpyldyLlMFg\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047f0a58>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('xpyldyLlMFg', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/0Hr5YwUUhr0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ff898>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('0Hr5YwUUhr0', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/ISHGyvsT0QY\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ffc50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('ISHGyvsT0QY', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's develop better intuition for how Convolutional Neural Networks (CNN) work. We'll examine how humans classify images, and then see how CNNs use similar approaches.\n",
    "\n",
    "Lets say we wanted to classify the following image of a dog as a Golden Retriever.\n",
    "\n",
    "<img src=\"img/dog-1210559-1280.jpg\" width=600>\n",
    "<center>An image that we'd like to classify as a Golden Retriever.</center>\n",
    "\n",
    "As humans, how do we do this?\n",
    "\n",
    "One thing we do is that we identify certain parts of the dog, such as the nose, the eyes, and the fur. We essentially break up the image into smaller pieces, recognize the smaller pieces, and then combine those pieces to get an idea of the overall dog.\n",
    "\n",
    "In this case, we might break down the image into a combination of the following:\n",
    "\n",
    "- A nose\n",
    "- Two eyes\n",
    "- Golden fur\n",
    "\n",
    "These pieces can be seen below:\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.49.08-pm.png\" width=200>\n",
    "<center>The eye of the dog.</center>\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.49.43-pm.png\" width=200>\n",
    "<center>The nose of the dog.</center>\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.50.54-pm.png\" width=200>\n",
    "<center>The fur of the dog.</center>\n",
    "\n",
    "## Going One Step Further\n",
    "But lets take this one step further. How do we determine what exactly a nose is? A Golden Retriever nose can be seen as an oval with two black holes inside it. Thus, one way of classifying a Retrievers nose is to to break it up into smaller pieces and look for black holes (nostrils) and curves that define an oval as shown below.\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.51.47-pm.png\" width=200>\n",
    "<center>A curve that we can use to determine a nose.</center>\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.51.51-pm.png\" width=100>\n",
    "<center>A nostrill that we can use to classify a nose of the dog.</center>\n",
    "\n",
    "Broadly speaking, this is what a CNN learns to do. It learns to recognize basic lines and curves, then shapes and blobs, and then increasingly complex objects within the image. Finally, the CNN classifies the image by combining the larger, more complex objects.\n",
    "\n",
    "In our case, the levels in the hierarchy are:\n",
    "\n",
    "- Simple shapes, like ovals and dark circles\n",
    "- Complex objects (combinations of simple shapes), like eyes, nose, and fur\n",
    "- The dog as a whole (a combination of complex objects)\n",
    "\n",
    "With deep learning, we don't actually program the CNN to recognize these specific features. Rather, the CNN learns on its own to recognize such objects through forward propagation and backpropagation!\n",
    "\n",
    "It's amazing how well a CNN can learn to classify images, even though we never program the CNN with information about specific features to look for.\n",
    "\n",
    "<img src=\"img/heirarchy-diagram.jpg\" width=600>\n",
    "<center>An example of what each layer in a CNN might recognize when classifying a picture of a dog.</center>\n",
    "\n",
    "A CNN might have several layers, and each layer might capture a different level in the hierarchy of objects. The first layer is the lowest level in the hierarchy, where the CNN generally classifies small parts of the image into simple shapes like horizontal and vertical lines and simple blobs of colors. The subsequent layers tend to be higher levels in the hierarchy and generally classify more complex ideas like shapes (combinations of lines), and eventually full objects like dogs.\n",
    "\n",
    "Once again, the CNN learns **all of this on its own**. We don't ever have to tell the CNN to go looking for lines or curves or noses or fur. The CNN just learns from the training set and discovers which characteristics of a Golden Retriever are worth looking for.\n",
    "\n",
    "That's a good start! Hopefully you've developed some intuition about how CNNs work.\n",
    "\n",
    "Next, lets look at some implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking up an Image\n",
    "The first step for a CNN is to break up the image into smaller pieces. We do this by selecting a width and height that defines a filter.\n",
    "\n",
    "The filter looks at small pieces, or patches, of the image. These patches are the same size as the filter.\n",
    "\n",
    "<img src=\"img/vlcsnap-2016-11-24-15h52m47s438.png\" width=600>\n",
    "<center>As shown in the previous video, a CNN uses filters to split an image into smaller patches. The size of these patches matches the filter size</center>\n",
    "\n",
    "We then simply slide this filter horizontally or vertically to focus on a different piece of the image.\n",
    "\n",
    "The amount by which the filter slides is referred to as the 'stride'. The stride is a hyperparameter which you, the engineer, can tune. Increasing the stride reduces the size of your model by reducing the number of total patches each layer observes. However, this usually comes with a reduction in accuracy.\n",
    "\n",
    "Lets look at an example. In this zoomed in image of the dog, we first start with the patch outlined in red. The width and height of our filter define the size of this square.\n",
    "\n",
    "<img src=\"img/retriever-patch.png\" width=600>\n",
    "<center>One patch of the Golden Retriever image</center>\n",
    "\n",
    "We then move the square over to the right by a given stride (2 in this case) to get another patch.\n",
    "\n",
    "<img src=\"img/retriever-patch-shifted.png\" width=600>\n",
    "<center>We move our square to the right by two pixels to create another patch.</center>\n",
    "\n",
    "What's important here is that we are grouping together adjacent pixels and treating them as a collective.\n",
    "\n",
    "In a normal, non-convolutional neural network, we would have ignored this adjacency. In a normal network, we would have connected every pixel in the input image to a neuron in the next layer. In doing so, we would not have taken advantage of the fact that pixels in an image are close together for a reason and have special meaning.\n",
    "\n",
    "By taking advantage of this local structure, our CNN learns to classify local patterns, like shapes and objects, in an image.\n",
    "\n",
    "## Filter Depth\n",
    "It's common to have more than one filter. Different filters pick up different qualities of a patch. For example, one filter might look for a particular color, while another might look for a kind of object of a specific shape. The amount of filters in a convolutional layer is called the *filter depth*.\n",
    "\n",
    "<img src=\"img/neilsen-pic.png\" width=400>\n",
    "<center>In the above example, a patch is connected to a neuron in the next layer. Source: Michael Nielse.</center>\n",
    "\n",
    "How many neurons does each patch connect to?\n",
    "\n",
    "Thats dependent on our filter depth. If we have a depth of k, we connect each patch of pixels to k neurons in the next layer. This gives us the height of k in the next layer, as shown below. In practice, k is a hyperparameter we tune, and most CNNs tend to pick the same starting values.\n",
    "\n",
    "<img src=\"img/filter-depth.png\" width=400>\n",
    "<center>Chossing a filter depth of k connects each patch to k neurons in the next year.</center>\n",
    "\n",
    "But why connect a single patch to multiple neurons in the next layer? Isnt one neuron good enough?\n",
    "\n",
    "Multiple neurons can be useful because a patch can have multiple interesting characteristics that we want to capture.\n",
    "\n",
    "For example, one patch might include some white teeth, some blonde whiskers, and part of a red tongue. In that case, we might want a filter depth of at least three - one for each of teeth, whiskers, and tongue.\n",
    "\n",
    "<img src=\"img/teeth-whiskers-tongue.png\" width=300>\n",
    "<center>This patch of the dog has many interesting features we may want to capture. These include the presence of teeth, the presence of whiskers, and the pink color of the tongue.</center>\n",
    "\n",
    "Having multiple neurons for a given patch ensures that our CNN can learn to capture whatever characteristics the CNN learns are important.\n",
    "\n",
    "Remember that the CNN isn't \"programmed\" to look for certain characteristics. Rather, it learns **on its own** which characteristics to notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Map Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/lp1NrLZnCUM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ffa20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('lp1NrLZnCUM', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the width, height and depth for padding = 'same', stride = 1?\n",
    "\n",
    "Enter your answers in the format \"width, height, depth\"\n",
    "\n",
    "Answer: 28, 28, 8\n",
    "\n",
    "<hr>\n",
    "\n",
    "What are the width, height and depth for padding = 'valid', stride = 1?\n",
    "\n",
    "Enter your answers in the format \"width, height, depth\"\n",
    "\n",
    "Answer: 26, 26, 8\n",
    "\n",
    "<hr>\n",
    "\n",
    "What are the width, height and depth for padding = 'valid', stride = 2?\n",
    "\n",
    "Enter your answers in the format \"width, height, depth\"\n",
    "\n",
    "Answer: 13, 13, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nf//Z\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/W4xtf8LTz1c\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ff7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('W4xtf8LTz1c', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Convolutions continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz\nODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2Nj\nY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQED\nEQH/xAAbAAEBAAMBAQEAAAAAAAAAAAAAAQMEBQIGB//EAEkQAAIBAgMEBQcIBwYGAwAAAAABAgME\nBRESEyExURQyYXGRFSJBUqGxwQYjM3KBorLRNUJTYmOCkiQ0c4Oz4RYlNkRkdCZU8f/EABgBAQEB\nAQEAAAAAAAAAAAAAAAABAgME/8QAIhEBAQEAAgECBwAAAAAAAAAAABEBIVExAkEDEhMiMmFx/9oA\nDAMBAAIRAxEAPwD8/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAADIZAAXIZAQFyGlgQF0sulgeQetLGlgeQetD7BofYB5B60PsG\nh9gHkHrZvsGzfYB5B72b7Bs32AeAe9m+wbN80B4B72UuaGylzQHgHvZS5ouylzQGMGTYy5obGXNA\nYwZNjLmhsZc0BjBk2MuaGwlzQGMGXYS5obCXNAYgZdhLmhsJc0BiBl6PPmh0efOIGIGXo8+cS9Hn\nziBhBm6PPnEdHnziBhBm6NPnEdGnziBhBm6NPnEdGnziIVhBn6LPnEdFnziBgBn6LPnEdFnziBgB\nn6LPnEdEqc4+IhWAGfolTnHxL0Spzj4liVrg2OiVOcfEdDqc4+IhWuDY6HU5x8R0Opzj4iFa4Njo\ndTnHxHQ6nOPiIVrlAIoAABQUAAAKAABSFAAFKAAAFAAAAIoAAFAAFCAAAAUAAUABFABVACgAABQA\nEAABQAAKABQAAAKAAKEAAUCgAACgAABygAYbCkKAKAAKQoAAoAAACpNvJLN9gM1tUjTc1KThqjkp\nRW9byrmXWHLJ5MGa5qRqVE4tyyilqfGXaYQbk16hGU5KMU5Sk8kl6Wb13hNazpylWr2yqQy1UVVT\nnH7DxhFxTtMWtLiss6dOrGUu7My4jhde3dW4dSjVoOWcasKsXrzfHLPMIUMGuK1GlUdS3o7bfShV\nqqMp9yMdrhd3d3lW0pU/n6UZOUJPJ7uK7zo3dnLFeiXNrWoKmrenTqaqqi6Tisnmn6PSZVf0p45i\nt1Qq6U7eapzzyzaUUmu/LMI4atartZ3GSVOE1Tlm9+bz/Iz+S7ryjKx0x20c29+7LLPPPlkdTELi\n2usBqXNKUIVq9eDq0VxU0pZtLk80/E9SvKHkLpqqLpk6KsnHPfknvl/TkgOTaYZWuaDr66NGipaV\nUrTUU3yXMSwy5hiFOylGO1qNKHnebLPg0+RuRoPEsGtKNtOntraVRTpymotqTTUlnx5GzCrSp4zg\n1DbQm7VQhUqKXm56m8s+SzyA4dOhOpcxoRS2kpqC3+nPIywsa069xRio67eM5T3+iPE2p21XDsTo\nXNxo2e3U/MqRluUs/QzflQ6Jc4ne1KtF0K9KrGlKNRNzc+GSW8DnWuD1bq3deFzaRjFapKVZJxWe\nW9eg1nZ1VQrVk4Sp0ZqnKUZZ5t55Zc1uNnDpxjh+JxlJJyoxUU3x89FoTgvk/dwckpuvTajnve6Q\nEoYPcVqNKo6lvS230catVRlPuRjtsLu7q8qWlOl8/TUnKEnk93Fd50L20lirtrm1q0dmqEKc1Ooo\n7JxWTzT9Hp3GR31OeNYnc0KuSdvJU555NtJJNeGYHGjbVHazuEls4TUHv35tNr3M9qxuHh7vlD5h\nT2bfada/r291gdS5pShGtXrwdWktzU0pZtLk80zZhc2EdGDy1ZbDYyqqotnrfnZ5Zetks8/QB870\nap0TpWS2W02eee/PLMxHYtrWpdYFKhSdN1YXepxlUjHdpyz3s5M4OnOUJZaovJ5PMo2q+G3NB2qq\nQS6VFSpPPc8//wBLQwyvXuLiipUqbt03UlUnpisnlx72dqld0K1zaWVepBUnRozp1G91Ooorj2Pg\n/sMVhNSxLGVBUasqkZaI1ZJRn84nzXoIOVDDpTupW8bm1zST1bVaX2J8zLUwa4p3cbVVKFSs9S0Q\nqJtNLNpmO4jKnikdrTo0Xqi3Gk04rhybM91dq3+UtW7ptSULlzTXpWoDV6DX8n9O0/MOps88/TkZ\nFhd076VppjtYR1TbklGCyzzb9HE+g2tj5T8lbaHQVTT1Z7tWrXx7txy7S6V9cYnGdSNOpexbhKby\nWepS059oGjdYdWtaUaznSq0ZPTtKU9Uc+XeZng10qCqKVGT2W12aqLXpyzzy7jNUpvD8Gr29xKDr\n3FWDjTjNScVHPe8uHE6DuraWmjSjShduwjGnXcvTp3x45J5ZrMI4NOyr1aNKpThrVWo6UEuLkknw\n+0x16LoVp0pSjKUHk3F5rxO5hd/QtcIhSrZJVa1SEpxfn0ouMVqRxbmg7a4nRc4z0PLVB5p9qKMR\nSFAAACgAAUAqABQAAAFAAFAA5IAMNqAUAAAKAgAKAAAKUAABQAAAARQAAKAAKAAAKAAAFAKEAAVQ\nAoAAAUABAAoAAAUAAUAAACgCkKEAAUUAACkKAAKAAAHJKAYbCgACgAACgAAAKAUAUAAAAKAECkKA\nAKAAAAoAApChAoBVAABQAAKQpEAAUCgACkKAKAAAAFAARQAUCkKAAAFAAAoAAoAHJAKYbAblhhtx\nfqboKGVPLU5TUUs+86NP5K39SLkp2yiuL28N3tA4YO5/w3OP0mI2MO+sn7h5Bto/SYzZLucn7kUc\nQHb8k4XHrY3S/lpTfwL5PwOPWxacvq0H8WQcMp2+j/J6PG8vJ/VpJfEf/HI//fn/AEoo4gO5t/k9\nHq2l3P61VL4DyhgcerhM5fWrv8gOIDt+VsLj1MFpfzVZv4iWMWmUdOEWqz+t+YHEB1sXo0Z3EXb0\n1ScqUJunHhvim8jlZEoAAqBQAKAABSFAAAIoBQAAKoUhQABQAACABQAAAoCKAAAApCgCkKEAClAA\nACkKAAKAAKAAAHKABht08Pb8lYik/wBWD+9/uerGcvI1/lJ5qVNrf2s8YZ51liEf4KfhKJcP34Zi\nMf3Iv7y/MDU6Rq3Vo6+3g/EbKFT6Kpv9WW5mApJ0PU4Sg8pJp9p5MsK84rTLKceUt56yo1ODdOXJ\n70L2MIMk6M6e9rzfQ1vTMZoCkKAPb6sDwe31IhG/i7ca1tJPfsKb+6jWaVws1uq8vW/3NnF+Fo+d\nvA56bTzRNxRrJ7wbCSuVyq/i/wBzC4STyaYzR5KMnyGT5FQAAAoAAoAQKAVQAAUAACgBAAACgACk\nKAKAAAAFAKEAAUCkKAAAFAAFAAAoAAAAcoApht0sG3q8jztp+zf8C4Vvt8Qjzt2/CUWecE/vFaPr\nW9RfdZ6wfrXkedtMDmgekACgFHunVnTfmyaMmulU68ND5x/IwAkHRtMGub2LnbOnOCeTk5qPvNxf\nJe8y86rQXdPV7szVt2/IFzk8sq0H7JHP1y9Zgdz/AIdcFnUq1Jf4VGUvfke44LSUFnQvp99NQXi2\ncFVJrhJ+J18QuKkcPw9t6k6TzT35+cxtHTvrK3VO12ltDdSy+duYxy3vlxNT+x0nu8nQ/rqM1cQj\nGtZ2LUtD2TSTe7rS9Jy5wnB5TTQzR3vKFnTecK8E/wCFaRXtbLUxyjCWf9orfWcI/Bnzx7nxXcio\n7cvlFFdWwoN/xIqXwR7pYisRtruNa0tqcYUtSdOkoteclx+0+fOjhX0N8v8Ax3+JAaNWm6c2nvXo\nfM8mem1Whsn1l1H8DA1k8mTNAApQAKEAAVQpCgAChAAAACgAABQCgAAAKAAKAVAAoAAACgACkKAA\nKAAAAoAHJKAYbdHAt+I6fWp1F9xnrBv71Xj61Covus84C8sXt1zlp8dx7wVf82UPWjOPjFgc18WQ\nsuswUAAAKAB0rTfgl8uUqb9rOadOw34TiK/dg/vI5oQOpiG/CMPf7s195nLOpe78DsHyc17QPF9n\n5Nw9/uSX3makK8orTLKUeTNu9/RNg+yf4jniUZ9nTq/Ry0y9WX5nmtCUJJSTTyRjRn27SjGSU45c\nGTnFYDo4RvjeL/x5fA1VRhVfzMspP9WR3MIwLEI7eVS3lTjOhOKc92/IZqPnU8nmjPUW2htV1l11\n8Tflg0aGfSbnS+UKcpfkhTeH281ppXNZ8Hqain9m8arklOlKrhzk87KtH6tX84nnVhj40bpf5ifw\nKjnlOhlhT9N1H+lhUMNlwua8e+kvzA54OvRwijcLOjcVZ91BmZfJ6DmoLEKG0e5Qb85/YUcMHurS\nlSm4yX28zwAKAEACgAAAKABQAAAKAAKEAAUCgADPTtK1S0qXUY/NU2oyefpZgO7b39lTp07FwzpO\nm4Tramlqlvby7Hl4AcNLN5I27nD69rurbPPVp0xqRk0+5M16WSrw1NZalm/tOhiN3RucUk4UqMI7\nfPaxcvOWfF5vIDXr4ddW7Sqwim5aclUi3nyyTMbtK6jWk6UsqDyqPLqvPLedDG5KV1OtTjaZSqtx\nnRqapS797OhO5tqteFtGrBQv4OdaWe6MnFZZ9zXtIPnKlKdLTtIuOqKks/Sn6TLTsrqrS2tK2qzp\n+tGDaPeJ143F/VnD6NPTD6q3L3G3Wq28MIsoVFVdTTNrZ1Eks5elZFHMcJKMZOLUZcG1uZXTmpOL\nhJOKzay4HT6NWvMNsFbU5VNDmp6d+luXp5bjYedziGLuhF1W6bjHQs8/OitwHDSbzyTeRMt2Z08O\nt60Y39N0aiqq3y0aXnva9BMKg3dysLmE4QuFpkmt8Wt6eQHzABTDbcwZ5YtaP+LH3mzhfmfKGmv4\nrRp4a9OJWz5VY+83bZaPlPBcrnL7xRy57py7yGS4Wm4qLlJmMAAUAAAOlh36OxFfw4v76OcdHC99\nriEf4Gf3onOCB1LrfgNm+VSa9xyzqVt/yetuytNeyIHm734PY9kqi9qOcdCvvwS17KtReyJoAD1P\n9XuPJ7nwj3AWi8q0O9HflWqxxzEIqpJLRVy39jPnqfXj3neqf9QXvbSqfgYHKjiN5Dq3VVd02ZYY\nvf5r+1VH3yzNF8Sx4oDfljF7qedVS74p/Anlau+tTt5d9GP5GlPrvvIB0FinrWdq/wDLy9xvXl7C\n1jb7G3pUpVKSm5xgm03nzOCdLF+pZf8ArR97G4PFevdXXG5lVXq6vgesG1LF7dSzz1+k56eR08Fr\nTeKW0Zecta4+gnODTdXTOUJrVDPgyTo+brpvVH2o91I0p1JZScHn6d6PMaVanJSgm+TjvJ/FYQdO\nOF3FxT2mx2PbN6YvuzMfki6fVVOXdVi/iazUjQKbrwi+9FBv6rTPEsMvY8bWt/Qyo1QZnaXEetRq\nLvizw6U1xi19gHko0vkMnyAAZAAUAAUAqABQN7YWtvb0Z3O1nUrR1qNNpKKzyXFPPgeaFtQ6M7m5\nnUVNz0RjTSzb4viI3sJW0KVxbRqummoT1OLS5dpLe7hC3dCvR2tPVrWUtLi+8KyeT4u/oUI1nsq8\nVOM3Hek+aPFewlQhXlKa+alFLJdZPgy+UH02ncbNKNOOiEE+CyyW8Tv3Uw2NrOGcoyTVTP8AVWeS\n9pB7eF1M7XKpFxuEt+XUb9D+wxW9oqtGpVnWjShCSjvTebefLuNq2xVUa6lKk5U9nGOnP9ZLczHZ\nXsKFrUoynXpuU1LVSy5ZZMDBRtZVYTqbSEKUXlrm2k32HqjZSrznGFaj5mfGWWaXpW4tGvRdrK3r\n7RR164ygk3nllvTPFvWhQqVXlJxlTlCPPeiosbSc4zkp0tMJadTmkm+wlO0r1KanCGcXnl5yzeXZ\nxM1tdU6dlOjKUoyc9Wapqae7L0vcWhc28IW0puproZvSorJvPPjmBpwU5PTDNvki09pvlT1buLiZ\nbWpGNw51HkmpLNLPLNGalcUKEHSWc1LPVPh3bgNeMq8W6kZVE+Dkmzy6tR1No5yc/Wz3nQeI09hs\n8pZ5ehZfE50nm2wOQUhTDbJbS03FOXKSZ0672Xypk+Vzn945VPdNd50sWejHqkv31L3Mo1cRjoxC\n4jyqSXtNY3caWWL3a/iy95pACgAAChHSwffG9jztpfBnNOjgv09ePO3qfhZz3xAh1Jb/AJOU+y4l\n+FHMOpHf8nJdlwvwgeKm/AqPZXl7kc86L3/J9dlw/wAJzgB7l1Y9x4PcurHuAkOuu879T/qC57aM\n/wDTOBHrLvO/L/qKp20H/pAcB8WVcUH1mFxAs+u+8h6n15d55KB0sW+isf8A14+9nNOliv0Nj/66\n97A5xvYJ+lrb66NE3sF/S1t/iIDUq/Sz7zoYBOUcWo5Nrj7maFb6WfezdwL9K0ft9zIPflm+nUe0\nra/rxUveY/K1fPzoUJd9GP5GpH6U8FR0PKrfXtbZ/wAmXuPccVpxf90gvq1Jr4nMAHZWNQ/ZVo/V\nuJI9xxml614v85S96OIAO95VtpLfOt/NRpyCvrOXGpD+a0h8GcIAd7bWEuM7R99GcfcZ9hhtJU3X\no0qjqLOMaUpLNd7Z80da8+kw1fwoe9k1XjEaFOhe1oU7d7OMmlvbNPXT/YrxZsYrOUcVutMmvnZc\nO811cT/Wyl3rMk0ptKf7JeLG0p/sY+LLtKcutSy+qxpoy4TlHvQDaU/2MfFljKMnlGgm+9npW8Ul\nKVSLT4JPLPxJJTa0w0RjyjJEuezrnwtl1610YdanGUuSb95jc4N/RR8WTY1M+o33bz1ojB/Ovf6q\n4/aOF+7qYsFCb3Un2vVwFWFNQUqbfHJniVRyWXCPJFf0EfrP4Fh82bm5HgAG3nUAACkKAAAFAAHK\nABhtY9ZHSxp54s5c4wf3Uc1cTpYz/e6MvWoU391FHnHV/wA4uHzlmc86OO/pOcvWhCXjFHPAAAIF\nIUDo4JvvZLnSqL7jOe+LOhgX6SgucJr7rNCXWYEOnS3/ACer9lePuZzDp22/AbpcqsH7JAeV+gJd\nlwvws550IPPAqq5V4+6RzwB7l1Ink9PqRKIuKO+9/wAol20F/pHDpUpTeaW5cW+CPo7RWd1i1GtC\n7jr2ai4OLW9Qy45Eo+afWZ7p0p1JJRidHyZrk9ld2ry/fy96LPDbnTooOnNPjKNWLb9pL0OdUhlU\nllKL3+hnnRI3JYRfx/7ao/qrP3GGdnc0+vQqR74tFGHRLkzo4qnsLDd/26/Ezn6Jx4xaOre0a9ej\nYRownKWw4JfvMDkm7gv6Wtv8RGZYfsd97cwo/uLzpeC+J6jiFrZyTs7fXUjwq1fgkS9EabtqtWrN\nwi9Kbzk9yX2mzZToYfdQrVJ7WUf1Yfmada5q15Z1Jt9noRhJN3ycOt0nDcsoU69LP0rTJmPRh8uF\n3Vj9aivzOcDWZEdHo1nLq39P+anJe7MqsIS6t3avvk170c0qKOj5Mm+rUt5fVrR/MeSbv0UHL6sk\n/cc/Nl1PmwNyWF3cVm7Wul9RmCVvODylGce+OR5jWqwecZyXczPHEr2KyV1Wy5a2Bg2fb7DtVbSr\nWuLFwjnGFGDk/Ql2s0Fi976a7l9ZJ+8tfFbm5hGnXkpwjwjlkl4DRMQgqt/cTjUpuMqkmnq7TX2H\n78P6i50ZeiUH4jYp9SpGXZnkzN1U2P8AEh4l2H8SHieZUqkOMWkeC89o2alFbKktpDg/T2mPYr9r\nAVPo6X1fiY+HEmZsdPifkzRorNfOw9p5r/Tz4dZnmG+cV2nqonKvJJZtyY9zz6J+3jIyTWVGmu1s\nZRpccpT5ehHiTcnnJ5svk49Gbm+UKQppyAABQAAKQoAAzR2GlalUz7GguZXEKQploOli+/oUudtH\n4o5p0cT329hL+Bl96QDG99zRl61Cm/uo550cZ42b520DnBAAoAAAdHAv0tQXNtexmjPrvvNzBHli\n9r/iJe01KyyrTXawPB1LPfgl6uU6b/Ec1Rb4I6uHwzwm/Wee6D3d4GGhvwO57K1P3SNFRb4I6lnS\nlPCbuMKbz1waz+0x07ZRaU1KrUfCnBZk31RY1qNpOq+S5mWorWilHOVSS5cDbqWlw4/2qrSs6fqy\nfnf0reYdeGW3VhUup85vRHwW/wBpJu+ThrefcyUIRm+UYxOlh9jKxuqdzeVIUIR36Zvzn9nE1KmK\n3Li4UXG3p+rSjp9vF/aaUpSk85Nt9pqIy1qqecKe6GfiYs3zIBmQe41JxecZNdzM0L+7h1bmqu6b\nNYpRvLF75bncTkuU/O95a2L3temqcqzjFLLKCUV7DQKBW2+LzIAEACgAABQCgAAAKQoApClQKQAe\n41Jw6smvtPar59eEZfZkYgSYtZ5VKU1FaJRyWW5nnKjzmvsRiKT5Y39Td84yxjSU09o9zz3xLOol\nKWz3Z8ZelmEoi/U2TMgAU05AAAFIUAAAKAAKAAOUADLYdK/34Zh8v3JR+8/zOadK534JZvlUqR/C\n/iAxXfb2Eudul4SZzjo4hvw3D5fuSj4Sf5nOCBQZqVtVqLNRyj6z3IDCe4U51HlCLb7DftsOlUfz\ndOddrjpWUV3s2J0KNKOVzd0qa/Z0Vrf5e0l6VqWWmzuqVeb1OnJS0x7O06FK2sb2pJ0aN2pN5vSl\nNL3Go7+0o/3a0U5evXer2cPeYLjEru5jpqVXo9EI7or7EMHVqYXh1JrbYnGDz3xcNUl4No3LV4Vb\nW1xClc0pqcV51Vt8H6qS+J8nnmUqPoLnGraNPZU1Urpeh/Nw/pRzquL3UouFJxoQf6tJafbxNAEz\nMwepSlJ5ybb7SEKUCgFAAAUAACgBAAACgACkKgBQAAAAoBQgACgUhQAAAoAAFAAFAAAACgAAUAAU\nhQABQjkgAy6KdGom8BovlXmvuxOcbtnidW1oOiqdKpTctWmpBPeBnuoSlg1j2SqLf3o8WuD3NwtS\npyUPWe5eLNjy9KNCNOlZ28JRbalp1ZZ5cE8+RoXOIXV2869ec+xvcvsJyOj0bD7P6a4jOS/VpLW/\ny9rMdXFaEX/Z7SMmuE671vw4I5JRCtm5xC6ut1atKUVwjnkl9hrAFRQAAKQpQAKQAChAAFUKQoAA\noAABApCgAABQEUAAABSFAFAKgAUAAABSFAFIUAAUAAABSFAAFAAACgAAUAqOSUhTDoAAIFAAFIUA\nUhQAAAoAAFQARQAVQAoAAACgBAAAUAACkKAKAAAAFAKEAAUCgAAABQABQAAKAAAKAAAAoAAoAAAp\nUAABygAYdApChAAoAAAUAACkKAAKEEUhQAAKoUhQABQgAABSFAAFAFIUAAABQABQCoAFAAAAUAAU\nhQBSFAAAAUAAUhQABQAAAoAKgAUDkgx7R9g2j7DDoygxbR8kNo+SCMpTDtZckNrLkgMxTDtZckNt\nLkgMwMO2lyQ20uSAzgwbaXJDbS5IDOU19tLki7eXJAjYBr7eXJDby5IUbANfby5IdIlyQo2Qa3SJ\n8ojpE+URRslNXpE+US9InyiKRsg1ukT5RHSZ8oikbQNXpM+UR0mfKIpG0VGp0mfKI6VPlEtSNwGp\n0qfKI6VPlElWNsGp0qfKI6VPlEUjcKaXS58ojpdTlHwLUjdBpdLqco+Bel1OUfAUjcKaXS6nKPgO\nmVOUfAUjdBpdMqco+A6ZU5R8BSN4Gj0ypyj4DplT1Y+ApG+DR6bU9WHgOm1PVh4CkbxTQ6bU9WHg\nOm1PVh4Ckb4NDptT1YeA6dV9WHgxSOgDn9Oq+rDwZenVfVh4MUjoA5/Tqvqw8GOnVfVh4MUjoFOd\n0+r6sPBjp9X1YeDFI6IOd0+r6sPBjp9X1YeDFSa6QOb5Qq+rDwZfKFX1YeDLcI6QOb5Qq+rDwY8o\nVfVh4P8AMUjUABhsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAf/Z\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/utOv-BKI_vo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1047ffcc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('utOv-BKI_vo', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, a \"Fully Connected\" layer is a standard, non convolutional layer, where all inputs are connected to all output neurons. This is also referred to as a \"dense\" layer, and is what we used in the previous two lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sharing\n",
    "\n",
    "<img src=\"img/vlcsnap-2016-11-24-16h01m35s262.png\" width=600>\n",
    "<center>The weights, w, are shared across patches for a given layer in a CNN to detect the cat above regardless of where in the image it is located.</center>\n",
    "\n",
    "When we are trying to classify a picture of a cat, we dont care where in the image a cat is. If its in the top left or the bottom right, its still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this?\n",
    "\n",
    "As we saw earlier, the classification of a given patch in an image is determined by the weights and biases corresponding to that patch.\n",
    "\n",
    "If we want a cat thats in the top left patch to be classified in the same way as a cat in the bottom right patch, we need the weights and biases corresponding to those patches to be the same, so that they are classified the same way.\n",
    "\n",
    "This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "Theres an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model.\n",
    "\n",
    "## Padding\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-10.05.37-pm.png\" width=300>\n",
    "<center>A 5x5 grid with a 3x3 filter. Source: Andrej Karpathy.</center>\n",
    "\n",
    "## QUIZ QUESTION\n",
    "\n",
    "Let's say we have a 5x5 grid (as shown above) and a filter of size 3x3 with a stride of 1. What's the width and height of the next layer?\n",
    "\n",
    "Answer: 3x3. We see that we can fit at most three patches in each direction, giving us a dimension of 3x3 in our next layer.\n",
    "\n",
    "<hr>\n",
    "\n",
    "As we can see, the width and height of each subsequent layer decreases in the above scheme.\n",
    "\n",
    "In an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simply add a border of 0s to our original 5x5 image. You can see what this looks like in the below image.\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-10.05.46-pm.png\" width=400>\n",
    "<center>The same grid with 0 padding. Source: Andrej Karpathy.</center>\n",
    "\n",
    "This would expand our original image to a 7x7. With this, we now see how our next layer's size is again a 5x5, keeping our dimensionality consistent.\n",
    "\n",
    "## Dimensionality\n",
    "From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given:\n",
    "\n",
    "- our input layer has a width of W and a height of H\n",
    "- our convolutional layer has a filter size F\n",
    "- we have a stride of S\n",
    "- a padding of P\n",
    "- and the number of filters K,\n",
    "\n",
    "the following formula gives us the width of the next layer: `W_out =[ (WF+2P)/S] + 1`.\n",
    "\n",
    "The output height would be `H_out = [(H-F+2P)/S] + 1`.\n",
    "\n",
    "And the output depth would be equal to the number of filters `D_out = K`.\n",
    "\n",
    "The output volume would be `W_out * H_out * D_out`.\n",
    "\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Quiz: Convolution Output Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For the next few quizzes we'll test your understanding of the dimensions in CNNs. Understanding dimensions will help you make accurate tradeoffs between model size and performance. As you'll see, some parameters have a much bigger impact on model size than others.\n",
    "\n",
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- With padding of size 1 (P)\n",
    "\n",
    "Recall the formula for calculating the new height or width:\n",
    "\n",
    "```\n",
    "new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer Output Shape\n",
    "What's the shape of the output?\n",
    "\n",
    "The answer format is HxWxD, so if you think the new height is 9, new width is 9, and new depth is 5, then type 9x9x5.\n",
    "\n",
    "Answer: 14x14x20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Solution: Convolution Output Shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "The answer is 14x14x20.\n",
    "\n",
    "We can get the new height and width with the formula resulting in:\n",
    "\n",
    "```\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "```\n",
    "\n",
    "The new depth is equal to the number of filters, which is 20.\n",
    "This would correspond to the following code:\n",
    "\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'SAME'\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "```\n",
    "\n",
    "Note the output shape of conv will be [1, 16, 16, 20]. It's 4D to account for batch size, but more importantly, it's not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch padding from 'SAME' to 'VALID' which would result in an output shape of [1, 13, 13, 20]. If you're curious how padding works in TensorFlow, read [this document](https://www.tensorflow.org/api_guides/python/nn#Convolution).\n",
    "\n",
    "In summary TensorFlow uses the following equation for 'SAME' vs 'VALID'\n",
    "\n",
    "**SAME Padding**, the output height and width are computed as:\n",
    "\n",
    "out_height = ceil(float(in_height) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width) / float(strides[2]))\n",
    "\n",
    "**VALID Padding**, the output height and width are computed as:\n",
    "\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "\n",
    "out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "NEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Quiz: Number of Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here!\n",
    "\n",
    "Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses.\n",
    "\n",
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape 32x32x3 (HxWxD)\n",
    "- 20 filters of shape 8x8x3 (HxWxD)\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "## Output Layer\n",
    "- 14x14x20 (HxWxD)\n",
    "\n",
    "## Hint\n",
    "Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron.\n",
    "\n",
    "### Convolution Layer Parameters 1\n",
    "How many parameters does the convolutional layer have (without parameter sharing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Solution: Number of Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution\n",
    "There are 756560 total parameters. That's a HUGE amount! Here's how we calculate it:\n",
    "```\n",
    "(8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560\n",
    "```\n",
    "\n",
    "`8 * 8 * 3` is the number of weights, we add 1 for the bias. Remember, each weight is assigned to every single part of the output `(14 * 14 * 20)`. So we multiply these two numbers together and we get the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Quiz: Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like you to calculate the number of parameters in the convolutional layer, if every neuron in the output layer shares its parameters with every other neuron in its same channel.\n",
    "\n",
    "This is the number of parameters actually used in a convolution layer ([tf.nn.conv2d()](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)).\n",
    "\n",
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape `32x32x3 (HxWxD)`\n",
    "- 20 filters of shape `8x8x3 (HxWxD)`\n",
    "- A stride of 2 for both the height and width (S)\n",
    "- Zero padding of size 1 (P)\n",
    "\n",
    "## Output Layer\n",
    "- `14x14x20 (HxWxD)`\n",
    "\n",
    "\n",
    "## Hint\n",
    "With parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So the number of parameters is equal to the number of neurons in the filter, plus a bias neuron, all multiplied by the number of channels in the output layer.\n",
    "\n",
    "### Convolution Layer Parameters 2\n",
    "How many parameters does the convolution layer have (with parameter sharing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Solution: Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "There are 3860 total parameters. That's 196 times fewer parameters! Here's how the answer is calculated:\n",
    "\n",
    "```\n",
    "(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860\n",
    "```\n",
    "\n",
    "That's 3840 weights and 20 biases. This should look similar to the answer from the previous quiz. The difference being it's just 20 instead of `(14 * 14 * 20)`. Remember, with weight sharing we use the same filter for an entire depth slice. Because of this we can get rid of `14 * 14` and be left with only 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Visualizing CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing CNNs\n",
    "Lets look at an example CNN to see how it works in action.\n",
    "\n",
    "The CNN we will look at is trained on ImageNet as described in [this paper](http://www.matthewzeiler.com/pubs/arxive2013/eccv2014.pdf) by Zeiler and Fergus. In the images below (from the same paper), well see what each layer in this network detects and see how each layer detects more and more complex ideas.\n",
    "\n",
    "## Layer 1\n",
    "\n",
    "<img src=\"img/layer-1-grid.png\" width=200>\n",
    "<center>Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).</center>\n",
    "\n",
    "Example patterns that cause activations in the first layer of the network. These range from simple diagonal lines (top left) to green blobs (bottom middle).\n",
    "The images above are from Matthew Zeiler and Rob Fergus' [deep visualization toolbox](https://www.youtube.com/watch?v=ghEmQSxT6tw), which lets us visualize what each layer in a CNN focuses on.\n",
    "\n",
    "Each image in the above grid represents a pattern that causes the neurons in the first layer to activate - in other words, they are patterns that the first layer recognizes. The top left image shows a -45 degree line, while the middle top square shows a +45 degree line. These squares are shown below again for reference.\n",
    "\n",
    "<img src=\"img/diagonal-line-1.png\" width=100>\n",
    "<center>As visualized here, the first layer of the CNN can recognize -45 degree lines.</center>\n",
    "\n",
    "<img src=\"img/diagonal-line-2.png\" width=100>\n",
    "<center>The first layer of the CNN is also able to recognize +45 degree lines, like the one above.</center>\n",
    "\n",
    "\n",
    "Let's now see some example images that cause such activations. The below grid of images all activated the -45 degree line. Notice how they are all selected despite the fact that they have different colors, gradients, and patterns.\n",
    "\n",
    "<img src=\"img/grid-layer-1.png\" width=200>\n",
    "<center>Example patches that activate the -45 degree line detector in the first layer.</center>\n",
    "\n",
    "So, the first layer of our CNN clearly picks out very simple shapes and patterns like lines and blobs.\n",
    "\n",
    "## Layer 2\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.09.02-pm.png\">\n",
    "<center>A visualization of the second layer in the CNN. Notice how we are picking up more complex ideas like circles and stripes. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>\n",
    "\n",
    "The second layer of the CNN captures complex ideas.\n",
    "\n",
    "As you see in the image above, the second layer of the CNN recognizes circles (second row, second column), stripes (first row, second column), and rectangles (bottom right).\n",
    "\n",
    "**The CNN learns to do this on its own.** There is no special instruction for the CNN to focus on more complex objects in deeper layers. That's just how it normally works out when you feed training data into a CNN.\n",
    "\n",
    "## Layer 3\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.09.24-pm.png\">\n",
    "<center>A visualization of the third layer in the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>\n",
    "\n",
    "The third layer picks out complex combinations of features from the second layer. These include things like grids, and honeycombs (top left), wheels (second row, second column), and even faces (third row, third column).\n",
    "\n",
    "We'll skip layer 4, which continues this progression, and jump right to the fifth and final layer of this CNN.\n",
    "\n",
    "## Layer 5\n",
    "\n",
    "<img src=\"img/screen-shot-2016-11-24-at-12.08.11-pm.png\">\n",
    "<center>A visualization of the fifth and final layer of the CNN. The gray grid on the left represents how this layer of the CNN activates (or \"what it sees\") based on the corresponding images from the grid on the right.</center>\n",
    "\n",
    "The last layer picks out the highest order ideas that we care about for classification, like dog faces, bird faces, and bicycles.\n",
    "\n",
    "## On to TensorFlow\n",
    "This concludes our high-level discussion of Convolutional Neural Networks.\n",
    "\n",
    "Next you'll practice actually building these networks in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. TensorFlow Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how to implement a CNN in TensorFlow.\n",
    "\n",
    "TensorFlow provides the [tf.nn.conv2d()](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) and [tf.nn.bias_add()](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add) functions to create your own convolutional layers.\n",
    "\n",
    "```python\n",
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "```\n",
    "\n",
    "The code above uses the tf.nn.conv2d() function to compute the convolution with weight as the filter and [1, 2, 2, 1] for the strides. TensorFlow uses a stride for each input dimension, [batch, input_height, input_width, input_channels]. We are generally always going to set the stride for batch and input_channels (i.e. the first and fourth element in the strides array) to be 1.\n",
    "\n",
    "You'll focus on changing input_height and input_width while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. This example code uses a stride of 2 with 5x5 filter over input.\n",
    "\n",
    "The tf.nn.bias_add() function adds a 1-d bias to the last dimension in a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Explore The Design Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/FG7M9tWH2nQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10aad7160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('FG7M9tWH2nQ', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. TensorFlow Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/max-pooling.png\" width=600>\n",
    "<center>By Aphex34 (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons</center>\n",
    "\n",
    "The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value.\n",
    "\n",
    "For example, [[1, 0], [4, 6]] becomes 6, because 6 is the maximum value in this set. Similarly, [[2, 3], [6, 8]] becomes 8.\n",
    "\n",
    "Conceptually, the benefit of the max pooling operation is to reduce the size of the input, and allow the neural network to focus on only the most important elements. Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.\n",
    "\n",
    "TensorFlow provides the [tf.nn.max_pool()](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function to apply [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) to your convolutional layers.\n",
    "\n",
    "```python\n",
    "...\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')\n",
    "```\n",
    "\n",
    "The tf.nn.max_pool() function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "\n",
    "The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1.\n",
    "NEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Quiz: Pooling Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few quizzes will test your understanding of pooling layers.\n",
    "\n",
    "QUIZ QUESTION\n",
    "\n",
    "A pooling layer is generally used to ...\n",
    "\n",
    "- (A) Increase the size of the output\n",
    "- (B) Decrease the size of the output\n",
    "- (C) Prevent overfitting\n",
    "- (D) Gain information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Solution: Pooling Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is decrease the size of the output and prevent overfitting. Preventing overfitting is a consequence of reducing the output size, which in turn, reduces the number of parameters in future layers.\n",
    "\n",
    "Recently, pooling layers have fallen out of favor. Some reasons are:\n",
    "\n",
    "- Recent datasets are so big and complex we're more concerned about underfitting.\n",
    "- Dropout is a much better regularizer.\n",
    "- Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Quiz: Pooling Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "- We have an input of shape `4x4x5 (HxWxD)`\n",
    "- Filter of shape `2x2 (HxW)`\n",
    "- A stride of 2 for both the height and width (S)\n",
    "\n",
    "Recall the formula for calculating the new height or width:\n",
    "\n",
    "```\n",
    "new_height = (input_height - filter_height)/S + 1\n",
    "new_width = (input_width - filter_width)/S + 1\n",
    "```\n",
    "\n",
    "NOTE: For a pooling layer the output depth is the same as the input depth. Additionally, the pooling operation is applied individually for each depth slice.\n",
    "\n",
    "The image below gives an example of how a max pooling layer works. In this case, the max pooling filter has a shape of `2x2`. As the max pooling filter slides across the input layer, the filter will output the maximum value of the `2x2` square.\n",
    "\n",
    "<img src=\"img/convolutionalnetworksquiz.png\" width=600>\n",
    "\n",
    "## Pooling Layer Output Shape\n",
    "What's the shape of the output? Format is HxWxD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Solution: Pooling Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "The answer is 2x2x5. Here's how it's calculated using the formula:\n",
    "\n",
    "```\n",
    "(4 - 2)/2 + 1 = 2\n",
    "(4 - 2)/2 + 1 = 2\n",
    "```\n",
    "\n",
    "The depth stays the same.\n",
    "Here's the corresponding code:\n",
    "\n",
    "```python\n",
    "input = tf.placeholder(tf.float32, (None, 4, 4, 5))\n",
    "filter_shape = [1, 2, 2, 1]\n",
    "strides = [1, 2, 2, 1]\n",
    "padding = 'VALID'\n",
    "pool = tf.nn.max_pool(input, filter_shape, strides, padding)\n",
    "```\n",
    "\n",
    "The output shape of pool will be [1, 2, 2, 5], even if padding is changed to 'SAME'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. Quiz: Pooling Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's practice doing some pooling operations manually.\n",
    "\n",
    "## Max Pooling\n",
    "What's the result of a max pooling operation on the input:\n",
    "\n",
    "```\n",
    "[[[0, 1, 0.5, 10],\n",
    "   [2, 2.5, 1, -8],\n",
    "   [4, 0, 5, 6],\n",
    "   [15, 1, 2, 3]]]\n",
    "```\n",
    "\n",
    "Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1.\n",
    "\n",
    "The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4.\n",
    "\n",
    "Work from the top left to the bottom right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25. Solution: Pooling Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "The correct answer is 2.5,10,15,6. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time.\n",
    "\n",
    "```\n",
    "max(0, 1, 2, 2.5) = 2.5\n",
    "max(0.5, 10, 1, -8) = 10\n",
    "max(4, 0, 15, 1) = 15\n",
    "max(5, 6, 2, 3) = 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26. Quiz: Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Pooling\n",
    "\n",
    "What's the result of a average (or mean) pooling?\n",
    "\n",
    "```\n",
    "[[[0, 1, 0.5, 10],\n",
    "   [2, 2.5, 1, -8],\n",
    "   [4, 0, 5, 6],\n",
    "   [15, 1, 2, 3]]]\n",
    "```\n",
    "\n",
    "Assume the filter is 2x2 and the stride is 2 for both height and width. The output shape is 2x2x1.\n",
    "\n",
    "The answering format will be 4 numbers, each separated by a comma, such as: 1,2,3,4.\n",
    "\n",
    "**Answer to 3 decimal places. Work from the top left to the bottom right**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Solution: Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "The correct answer is 1.375, 0.875, 5,4. We start with the four numbers in the top left corner. Then we work left-to-right and top-to-bottom, moving 2 units each time.\n",
    "\n",
    "```\n",
    "mean(0, 1, 2, 2.5) = 1.375\n",
    "mean(0.5, 10, 1, -8) = 0.875\n",
    "mean(4, 0, 15, 1) = 5\n",
    "mean(5, 6, 2, 3) = 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAAAAH/2Q==\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/Zmzgerm6SjA\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10bc4e3c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('Zmzgerm6SjA', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29. Inception Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz\nODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2Nj\nY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQED\nEQH/xAAbAAEBAAMBAQEAAAAAAAAAAAAAAQMEBQIHBv/EAEQQAQACAQEDBgoHBwMDBQAAAAABAgME\nBRFREhMUIZGxFTE0QVJTYXGSoSIyNVRictEGM0Njc4GiI8HhgpOyJERkg/D/xAAZAQEBAQEBAQAA\nAAAAAAAAAAAAAQIDBAX/xAArEQEAAgEDAgUDBAMAAAAAAAAAARECAxIxIVEEEzJBcSIz8FJhscEj\nYpH/2gAMAwEAAhEDEQA/APn4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbjcAG5\ndwILuOTIILyZOTIIPXJk5Mg8j1yJ9hyJ9gPI9cifYcifYDyPXNz7Dm59gPI983PsObn2A8D3zc+w\n5ueMA8D3zU8YOatxgHge+at7F5q3sBjGTmrew5m3sBjGTmbcYOZtxgGMZOZtxg5i3GAYxl5i3GDm\nLcYBiGXmLcYOYvxgGIZej34wdHvxgGIZej34wvR78agwjN0e/Gp0e/GoMIzdGvxqdGvxqDCM3Rr8\nanRr8alFsIzdFvxr2r0W/GvaDAM/Rb8anRb8agwDP0W/GvadEyca9pRbAM/RMnGvadEyca9q0lsA\nz9Eyca9q9Eyca9pRbXGx0PJxr2nQ8nGvaUW1xsdDyca9p0PJxr2lFtdQRQABRQAAUABUUAFUAAFA\nAARQAFABQAABR7x4r5Z3Y6TaeEQ2a7L1dqzacU1iPS6u9amWcs8ceZaatvwbqPwfHH6r4O1H4Pjj\n9VqWfMw7tQbfg7Ufg+OP1PB2o4U+OP1Kk8zDu1Bt+DtRwp8cfq9V2Zqr2itaVmZ8URaCpPNw7tMe\nsmO2LJal43WrO6YeUb5UAAAFAAUAUAAFAVFEAFFAABQAAcoBhsVFAUAFRQAUAABQUAAUABUUQBQA\nUAABQBv6DLfDpdTfFaa2iK9ce9l0+t1NtJqbWzXmYiN07/a19N5DqvdXvNL5HqfdHe08+WMTczHv\nDz07Veuv2nTtV66/a1xLddmPZsdO1Xrr9q9P1Xrr9rWCzZj2bPT9V66/a6f7ParPl2vhpkyWtXr6\npn2OI637M/bOL3T3NYz1hx8RhjGll09mltDy/Uf1Ld7WbGu69bn/ADz3sDM8u2HpgAGhUUAFAVFA\nAAUAFBUBQAABQAUAckBhtQUAAFAAUAFRVAABQAAEUABQAUAAUAAG5pfIdV7q95pfI9V7o7zS+Q6r\n3V710vkeq90d7TjPv8x/TUAR2AUB1/2X+2Mf5bdzkOx+y/2vT8tu5rHmHDxP2cvhztZ5Xm/PPews\nuq8qy/mliZl1x4gBRQAFBQAAFRQFRRABRQAAUAFAAByVRWGxQAUAAUAAFAjrUFbGn0Gp1P7rDa0e\ned3VH92z4OwYfKtXSJ9DF9Of0+YOczYNLn1FuThxXvP4Y3urp8VJjfo9nzeI/i6ier/aFz56xXk6\nvX76+p01ert6o7wangfPERy8mClvRtlrEx82fF+zmuy15WOcNq8Yy13MdtpY8NI6JpqUn08n07fP\nq+Tzo9ZqNRtTTWzZbX/1K+OfaI0L1ml5rPjid0oy6uOTqsscLyxAECgAAKACoojc0vkOq91e80vk\neq90d5pvItV7q95pfI9T7o72nKff5hqAI7KAA7P7Lfa0fkt3OO7P7Lfan/127msPVDz+K+zl8OVq\nOvPk/NLG95/31/fLwy7RwKAoqKAoAAAoAigKCooAAKAAoAKAOSCsNgAKACgAKiqDp7DvXFl1GWce\nPJbHim1YvXfG+PY5jo7I+vqY46e/cDNm2/qM3Vlw4bx5omvVH9mXS66Mmk1WSNJpq5MVYtW0Y/F1\nxHncSfG6GzevTa6P5O//ACgGvqNZqNTO/NltbhEz1QwIoj1P7uPezaCd2uwT+OO9hn93HvZNJO7V\nYp/FAPe0I3a/PHDJbva7a2pG7aep/q272qCgAKigAoAKI29N5FqvdXvNL5HqfdHeabyPVe6O80vk\nep90d7TlPv8AMNRUVHYBQHa/Zb7St7MdnFdv9lftDJ/Ss1h6oebxX2cnHzfvbe94e8n7y3veWXeO\nAAVQUAABUUBUUQBVAABUUBUUAFAAByVBhsUAAUABRQAHQ2P16nJXjhyR/jLnuhsTdO0IifPS8f4y\nDRnxy39ldePWx/8AHnvhoW+tLf2Ru5WpjjgsDQDzgj3/AA/7rp53Z6T+KE/h/wBzF1Za+8Gztb7U\n1P8AUt3tVubY+1NR7bzLTAABQAFAQUFVtabyTU+6O9dL5JqfdHeaXr0mp90d7LW8aHByJrW+XJG+\n1beKI8zThlPMR3aA2umx92w/CvTK/dsPZKdG92XZqK2ul0+64eyf1XpmP7rh+f6lQbsuzUdv9lvL\nc0/ybOd0vH91xfP9XW2DqcfOam0YaU5OG0743tY8uHiZmdLKKcG/1596LbrtMow9MCooooAAAoKI\nAKCgAACgAoACgAADlArDYACgKCgAACt7Ys7tqYPbbc0W5sid21NN/Ur3g1ckbslve3tjeU5I44bx\n/jLTzxuz5I/FLc2L5fER56Xj/GRGhPjke4x3vaeTWZ3cIXmcvq7di1K1KR+7n3pT68e97nHemOZt\nWY6/PDxX60FUTFN7bX2pmmPPun5NFv7a+0bTHnrWf8YaCIKigAogCgAKrf2ZnwYedjPO7fH0erf1\nvOSmnyZJvbVTMzO+d9Gkq25eX9W6JbfMab71/hJ0fT+bVV+GWopa7Z7tro2D71Tslei4fvePsn9G\noFm2e7b6Ji+9Yvn+ja0s4tHp9T/r0vbJj5ERXfxhylLTLTnKKmQFR0AUAABUUBQVAFAAAUAFRQAU\nAABfMAOSoMNiooACigAKADa2ZO7aOnn+ZXvarNo7cnV4rcLxPzBdZHJ1maOF572zsX7TxRx3x8pY\ntqV5O0tTHDJbvZNjbvCmn3+e24R7nNyIvjild0UifP1+Jgrqd9ojm6+PjP6s16TbLfzRzcRvlrRp\n8kWj6vxQ9OU59KerKc+lO1t2laaLHWkRFa33RDgx44fodvxu0Vd+7fNomOv2PzseM8V9xnxPrb22\nPLKzxxU/8YaLf2vH+vhnjhp/4w0HmedQAFARQFUBQAAFAQABQAFAFeqUtffyfNxnc8suGeTXJPVP\n0fP74SW8IicqlOZv+H4oOYvwj4oOdn0KfCvO/gp8KdW/8Sczk4R2wvMZPROd/BTsXnf5dOw6laSc\nzk9CV5nJ6Fuw5yPV0+Zzkerp8/1XqVpd05rJ6FuxeayehbsOcj1de2f1XnY9XXtn9TqbdPu8zjvE\nb5paI9sPLqbJ5GW+aLY4mvN7p654w5k+NIyuZhNTTjHGMo9wBtxUAFAABQAAcoFYbAFBQABQAAGT\nDO7NSfaxvVPrx7xG5tiN21dT7bzLzsqd209PP8yO9k21G7aebd590/Jj2ZS9toYORWbTF4nq95wL\nqNRkrmvj6prW0xETWGLpFvRp8MN3XbOzdOz7px/Xn+JHFjx7I1eW3JxUred2/dW8Sef+7puza+TP\nk1E2tltNp3MMeN7iN3KifHEPELMzPWWJmZm5dDa8bsmnnjgp3NBv7V/9rPHT1aCIKiiCoooAoKAD\nbxaDJkwRmm1KUtO6JvO7ew6fDOfPTFXx2nc7Gu0ubLpKUx1jkUvMVjlR4t0LEOGpqbZjG3O6BbzZ\nsPxweD8nrMP/AHIPB+q9CPig8H6qP4c9sBu/2PB+X0sX/cr+p4Pz/wAv/uV/VOgav1VjoOrj+Dk7\nA3T+qF8H6jzRX44J2fqYrNuRviI3zumJToWr9Tk+GW/szTZsdNXfLjtWK4LeOCIZz1Jxi7hyFJ8Y\nj0DLj/dZPdHexPeO8Vi0TXlRPtSW8JiJ6vO6TdLJy8fq5+I5WP1c/EWbY7vAycrH6Fvi/wCDlYvR\nv8RZsjuxq978fC/au/F+P5LZsjuxjJvx8bn+nxv2Fmz93Q2N1RqLcKOY2tNqa6fl8mbTF6zExuar\nOMfVMumpMeXjjHtYoNvOAoAACgACg5KorDYCqAACgACiC1+tCPVPr194OttLS2z6++S0xTFyaza8\n+KPowxYNXTHqMeLS1mtOVHKtP1rf/uDe27rOa1OPFzOO9IxUmOVv4R7XPwazHz1P/SYY+lHXG/8A\nVx+rKLmG+kN7PsrU7S29qaYo3UjJPKvPirG9+r2boNPs3DOLTxvtaPp5J8dv+G3mrXHya46xWLRF\np3eeZ87xj+vD5mrr5ZxXs9mnpREbpfNLxuy5I97FDPnjdqs0e2WF9iOHglv7T68Wjn+RHfLQb+0O\nvSaGf5Ux/lLQUUARQFUBQAAbWzftDD+eGXNM+D69f8W3dDFs3y/B+eGTN9n1/q27oV58/XH53afK\nnjK8q3GXkR3p7i9vSk5y/pT2vIJUMnPZPTt2upsi97aXaHKtM7sE+OfbDkOtsfyLaM/yf94ajlw1\n4jy/+fy5QDL0AKAqKIAKKAACgAoAAKAAqKAACgA5QDDagKAKAAAoCC1+tCLHjgHX/aLr1Ontx09J\n+Tl4f3tfe6m3+voNuOmr/u5WP69fexj6Vnl9SzTvphtxx1n5PGL95X3rM8rSaW3HDXuTH9evvfDn\n3fTw9EPnOrjdr9RH4rNZu7QrFdp6nf1RGS0fNq0pF7RWu+ZnxRufex9MPmTy3Nd17P0P5LR/lLRd\nHacVw6fS6abb8mKs8uOG+d+5zlQVFAAUFABUURs7N8vwfnhkz/Z9f6tu6GPZ3l+H88Nro982giKz\nWP8AVtP0rRHmhXDOYjO5/OXMVteD8/4J914XwdqfRif+qEp08zHu1Bt+DtT6v5wng/VeqlTzMO7W\ndbZHkG0f6Ud8NKdBqo/g37HS2fgy4dm7QnLS1d+OIjfHtWOXHXyxnCont/LigMvSKAKAqCooAACg\nAoAKigAAoACooAKDkqistgAKAACiAAKQAOvtvr02zrcdPHfLlV+tHvdba3XsvZtv5cx/lLkx44Yw\n9Kzy+nYp5WzdFbjhr3PWOPpRM+Le8aGs32PobeaMUPe602jqnc+JlzL6WnP0Q+fbaifDGqrHrLbu\n1lpFdl4YvaN+svG+serjj729tbHj0G09Tqs0RbNa8zixz5vxS4GXJfLktfJabWtO+Zl9vT9EPm5c\npa03tNrTvmfOgrbIAqiooAAKAI2tmxv2hgiPThk2lbk5owV8WKN39/O1cGW+DLXLjndas74lsTtD\nJa02tjwzM8ccK5ZY5b90NXfPtN88ZbXTt/j0+Gf+hemUnx6XD2T+o1eXZq754ycqeMtrpeL7pi7b\nfqdJw+fSY+236ibp/S14vb0p7XS0WS0bJ12+0zvisfNqxqNP59JX4per6ynRr4cWGMcXmJmd8z4i\nGM4nOKppqio7ioqoAAoACooAKACgAAKigAAoAKADkqDLYoAAAKAgqKAqKDr7R69hbOnhy4+cNTSa\navI6Rqd9cNfFHnvPCHWx6aNTsHSXvEzTFe++K+OfF1OXq66rUXiZw2rSvVWsV6ohxjK/pbmPd+/2\nfqLW2No70+hE08Uebredp7XjZWjnNltysto/08c+f2y1dFrcOzv2X0ufU/WpForj89p3y/FbS1+b\naOqtnzW3zPijzRDxafh9+pMzw7ZakRhERyx6zVZdbqb581pte075lhRX04iukPKKCqAAoACgIAAM\nkYckxExS0xPseI8bc1HNxzcTyo3V3dTphjExMy6YYRMTMtfmcnoW7DmsnoW7Hrfi9PJH9l309bk7\nP+V24rtxY+bv6Nuw5M8JZYtX114/t/ysXj7xfsNsGzFh3SbpbHLn7xPZJF53+UfKTZBshrjNqo3a\ni8cJYmMoqac8o2zQAjIqKAACgAoACgAACgAKACooAAjlAMuigAKiiAAKACgA2MOt1OnpyMOe9K8K\n23MsbW1/3rL8UtJU2wWz6nW6jVxWM+W1+T4t8sAqxFcACiACqKigAogAAqKBDc1NOVNPp1j6Pnlp\nwz3zxed98cTO7dv3y64TERMS64ZRGMxLzzP46fEvMz6VPig5eP1UdsnLx+rn4isSsDmbca/FBzF+\nEdsHKxehb4v+F5WH0b/EViVinMZOHzhYw5OVH0ZN+HhfthYnDv3/AE/kRGJEY29azyrJ+ZhestuX\nktaPPO95Zzm8pYzm8pmAFZYAAFRQFRQAUAABUUAFAABQBBQUclUVh0ABBQAVFAVFAABQAFgBFAVQ\nFAAAUBAAFAAVFAUAAUAFEAFBQABQAAUAFAABQAAUABQABRABRygGHQVFEAUAFAAAVFABRCFRQAFU\nUAAUAAQVFABQAUAABUUBQVAFAAAUAFRQFRQAAFABUUAFAAEFBQBQckY+cn2HOT7GHRlGLnJ9hzk8\nIEZVYedtwg523CAZlYOdtwhedtwgGYYedt7DnrcIBnGDnrcIOetwgGdYa/PW4Qc/bhAU2Rr8/bhB\nz9uEFjYGvz9uEHP34QWNka3SL8IOkX4QWNlWr0i/Cq9IvwqWU2Rq9IvwqvSL8KllNoavSb8KnSb8\nKllNpWp0m/Cp0m/Cq2lNwanSr8KnSr8Kpa02xqdKvwqdKvwr2LZTcGn0rJwr2HS8nCvYWlN0aXS8\nnCvYvS8nCvYWU3FaPS8nCvYvTMnCvYWU3RpdMycK9h0zJwr2FlN4aPTMnCvYdMycK9hZTfGh03Jw\nr2L03Jwr2FlN5Wh03Lwr2HTcvCvYWU3xodNy8Kdh07Lwp2FlOgOf07L6NOw6dl9GnYWU6I5/Tsvo\n07Dp2X0adhZToK53T8vo07Dp+X0adkllOiOd0/L6NOyTp+X0adkllOkOb0/L6NOyV8IZfRp2Stwl\nOkOb4Qy+jTsk8IZfRp2SXBTUAYbAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAf/Z\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"720\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.youtube.com/embed/SlTm03bEOxA\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10bc40080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('SlTm03bEOxA', width=720, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30. Convolutional Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to walk through an example Convolutional Neural Network (CNN) in TensorFlow.\n",
    "\n",
    "The structure of this network follows the classic structure of CNNs, which is a mix of convolutional layers and max pooling, followed by fully-connected layers.\n",
    "\n",
    "The code you'll be looking at is similar to what you saw in the segment on **Deep Neural Network in TensorFlow** in the previous lesson, except we restructured the architecture of this network as a CNN.\n",
    "\n",
    "Just like in that segment, here you'll study the line-by-line breakdown of the code. If you want, you can even [download the code and run it yourself](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61ca1_cnn/cnn.zip).\n",
    "\n",
    "Thanks to [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples) for providing the original TensorFlow model on which this segment is based.\n",
    "\n",
    "Time to dive in!\n",
    "\n",
    "## Dataset\n",
    "You've seen this section of code from previous lessons. Here we're importing the MNIST dataset and using a convenient TensorFlow function to batch, scale, and One-Hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "# epochs = 10\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Number of samples to calculate validation and accuracy\n",
    "# Decrease this if you're running out of memory to calculate accuracy\n",
    "test_valid_size = 256\n",
    "\n",
    "# Network Parameters\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75  # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "<img src=\"img/convolution-schematic.gif\" width=400>\n",
    "<center>Convolution with 33 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution</center>\n",
    "\n",
    "The above is an example of a [convolution](https://en.wikipedia.org/wiki/Convolution) with a 3x3 filter and a stride of 1 being applied to data with a range of 0 to 1. The convolution for each 3x3 section is calculated against the weight, [[1, 0, 1], [0, 1, 0], [1, 0, 1]], then a bias is added to create the convolved feature on the right. In this case, the bias is zero. In TensorFlow, this is all done using [tf.nn.conv2d()](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) and [tf.nn.bias_add()](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tf.nn.conv2d()](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) function computes the convolution against weight W as shown above.\n",
    "\n",
    "In TensorFlow, strides is an array of 4 elements; the first element in this array indicates the stride for batch and last element indicates stride for features. It's good practice to remove the batches or features you want to skip from the data set rather than use a stride to skip them. You can always set the first and last element to 1 in strides in order to use all batches and features.\n",
    "\n",
    "The middle two elements are the strides for height and width respectively. I've mentioned stride as one number because you usually have a square stride where height = width. When someone says they are using a stride of 3, they usually mean tf.nn.conv2d(x, W, strides=[1, 3, 3, 1]).\n",
    "\n",
    "To make life easier, the code is using [tf.nn.bias_add()](https://www.tensorflow.org/api_docs/python/tf/nn/bias_add) to add the bias. Using [tf.add()](https://www.tensorflow.org/api_docs/python/tf/add) doesn't work when the tensors aren't the same shape.\n",
    "\n",
    "## Max Pooling\n",
    "\n",
    "<img src=\"img/maxpool.jpeg\">\n",
    "<center>Max Pooling with 2x2 filter and stride of 2. Source: http://cs231n.github.io/convolutional-networks/</center>\n",
    "\n",
    "The above is an example of [max pooling](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) with a 2x2 filter and stride of 2. The left square is the input and the right square is the output. The four 2x2 colors in input represents each time the filter was applied to create the max on the right side. For example, [[1, 1], [5, 6]] becomes 6 and [[3, 2], [1, 2]] becomes 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [tf.nn.max_pool()](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function does exactly what you would expect, it performs max pooling with the ksize parameter as the size of the filter.\n",
    "\n",
    "## Model\n",
    "<img src=\"img/arch.png\">\n",
    "<center>Image from Explore The Design Space video</center>\n",
    "\n",
    "In the code below, we're creating 3 layers alternating between convolutions and max pooling followed by a fully connected and output layer. The transformation of each layer to new dimensions are shown in the comments. For example, the first layer shapes the images from 28x28x1 to 28x28x32 in the convolution step. Then next step applies max pooling, turning each sample into 14x14x32. All the layers are applied from conv1 to output, producing 10 class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer - 7*7*64 to 1024\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output Layer - class prediction - 1024 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "Now let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch   1 -Loss: 55692.5469 Validation Accuracy: 0.042969\n",
      "Epoch  1, Batch   2 -Loss: 47510.9531 Validation Accuracy: 0.093750\n",
      "Epoch  1, Batch   3 -Loss: 43780.7578 Validation Accuracy: 0.101562\n",
      "Epoch  1, Batch   4 -Loss: 37607.2148 Validation Accuracy: 0.109375\n",
      "Epoch  1, Batch   5 -Loss: 33504.0977 Validation Accuracy: 0.113281\n",
      "Epoch  1, Batch   6 -Loss: 32436.1250 Validation Accuracy: 0.128906\n",
      "Epoch  1, Batch   7 -Loss: 25869.0742 Validation Accuracy: 0.136719\n",
      "Epoch  1, Batch   8 -Loss: 28617.6406 Validation Accuracy: 0.167969\n",
      "Epoch  1, Batch   9 -Loss: 25578.6562 Validation Accuracy: 0.179688\n",
      "Epoch  1, Batch  10 -Loss: 23613.3164 Validation Accuracy: 0.175781\n",
      "Epoch  1, Batch  11 -Loss: 22770.6484 Validation Accuracy: 0.167969\n",
      "Epoch  1, Batch  12 -Loss: 22180.4824 Validation Accuracy: 0.164062\n",
      "Epoch  1, Batch  13 -Loss: 24596.3184 Validation Accuracy: 0.187500\n",
      "Epoch  1, Batch  14 -Loss: 19727.6953 Validation Accuracy: 0.210938\n",
      "Epoch  1, Batch  15 -Loss: 18506.2383 Validation Accuracy: 0.226562\n",
      "Epoch  1, Batch  16 -Loss: 16395.6758 Validation Accuracy: 0.226562\n",
      "Epoch  1, Batch  17 -Loss: 17251.0977 Validation Accuracy: 0.222656\n",
      "Epoch  1, Batch  18 -Loss: 16339.1484 Validation Accuracy: 0.230469\n",
      "Epoch  1, Batch  19 -Loss: 15561.0039 Validation Accuracy: 0.250000\n",
      "Epoch  1, Batch  20 -Loss: 14948.9336 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  21 -Loss: 16587.9922 Validation Accuracy: 0.269531\n",
      "Epoch  1, Batch  22 -Loss: 14603.8477 Validation Accuracy: 0.285156\n",
      "Epoch  1, Batch  23 -Loss: 14964.5508 Validation Accuracy: 0.304688\n",
      "Epoch  1, Batch  24 -Loss: 13153.6758 Validation Accuracy: 0.308594\n",
      "Epoch  1, Batch  25 -Loss: 13100.0371 Validation Accuracy: 0.300781\n",
      "Epoch  1, Batch  26 -Loss: 13494.4365 Validation Accuracy: 0.316406\n",
      "Epoch  1, Batch  27 -Loss: 12833.4697 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  28 -Loss: 14995.5869 Validation Accuracy: 0.339844\n",
      "Epoch  1, Batch  29 -Loss: 13837.3906 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  30 -Loss: 14569.1260 Validation Accuracy: 0.335938\n",
      "Epoch  1, Batch  31 -Loss: 10613.9814 Validation Accuracy: 0.343750\n",
      "Epoch  1, Batch  32 -Loss: 12499.7070 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  33 -Loss: 11723.7812 Validation Accuracy: 0.359375\n",
      "Epoch  1, Batch  34 -Loss:  9829.3643 Validation Accuracy: 0.371094\n",
      "Epoch  1, Batch  35 -Loss: 13536.5420 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  36 -Loss: 11706.9805 Validation Accuracy: 0.359375\n",
      "Epoch  1, Batch  37 -Loss: 10063.9033 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  38 -Loss: 10446.1660 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  39 -Loss:  8973.9307 Validation Accuracy: 0.355469\n",
      "Epoch  1, Batch  40 -Loss: 11216.3721 Validation Accuracy: 0.347656\n",
      "Epoch  1, Batch  41 -Loss: 11027.7168 Validation Accuracy: 0.351562\n",
      "Epoch  1, Batch  42 -Loss:  9336.1465 Validation Accuracy: 0.355469\n",
      "Epoch  1, Batch  43 -Loss:  9605.5010 Validation Accuracy: 0.382812\n",
      "Epoch  1, Batch  44 -Loss:  8368.9463 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch  45 -Loss: 10079.4541 Validation Accuracy: 0.363281\n",
      "Epoch  1, Batch  46 -Loss:  9860.7578 Validation Accuracy: 0.367188\n",
      "Epoch  1, Batch  47 -Loss: 10403.1436 Validation Accuracy: 0.375000\n",
      "Epoch  1, Batch  48 -Loss:  8996.4355 Validation Accuracy: 0.386719\n",
      "Epoch  1, Batch  49 -Loss:  8724.0332 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  50 -Loss:  9386.0117 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  51 -Loss:  9467.5195 Validation Accuracy: 0.398438\n",
      "Epoch  1, Batch  52 -Loss:  8197.7324 Validation Accuracy: 0.394531\n",
      "Epoch  1, Batch  53 -Loss:  7487.6523 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  54 -Loss:  8430.5234 Validation Accuracy: 0.410156\n",
      "Epoch  1, Batch  55 -Loss:  8694.9180 Validation Accuracy: 0.414062\n",
      "Epoch  1, Batch  56 -Loss:  8341.6953 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  57 -Loss:  8152.6143 Validation Accuracy: 0.417969\n",
      "Epoch  1, Batch  58 -Loss:  7745.5459 Validation Accuracy: 0.429688\n",
      "Epoch  1, Batch  59 -Loss:  7021.6069 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  60 -Loss:  8617.9043 Validation Accuracy: 0.437500\n",
      "Epoch  1, Batch  61 -Loss:  7629.3374 Validation Accuracy: 0.445312\n",
      "Epoch  1, Batch  62 -Loss:  6913.6304 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  63 -Loss:  8578.2383 Validation Accuracy: 0.441406\n",
      "Epoch  1, Batch  64 -Loss:  6096.3828 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  65 -Loss:  6817.3794 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch  66 -Loss:  7434.2852 Validation Accuracy: 0.464844\n",
      "Epoch  1, Batch  67 -Loss:  7449.7148 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  68 -Loss:  5297.8076 Validation Accuracy: 0.449219\n",
      "Epoch  1, Batch  69 -Loss:  6182.2900 Validation Accuracy: 0.453125\n",
      "Epoch  1, Batch  70 -Loss:  5518.0063 Validation Accuracy: 0.468750\n",
      "Epoch  1, Batch  71 -Loss:  6785.1875 Validation Accuracy: 0.460938\n",
      "Epoch  1, Batch  72 -Loss:  5868.0996 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  73 -Loss:  6469.8574 Validation Accuracy: 0.484375\n",
      "Epoch  1, Batch  74 -Loss:  6418.8931 Validation Accuracy: 0.488281\n",
      "Epoch  1, Batch  75 -Loss:  6219.8179 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  76 -Loss:  5875.2983 Validation Accuracy: 0.492188\n",
      "Epoch  1, Batch  77 -Loss:  5620.3643 Validation Accuracy: 0.480469\n",
      "Epoch  1, Batch  78 -Loss:  5930.8623 Validation Accuracy: 0.500000\n",
      "Epoch  1, Batch  79 -Loss:  6599.2246 Validation Accuracy: 0.484375\n",
      "Epoch  1, Batch  80 -Loss:  6193.6968 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  81 -Loss:  5544.4888 Validation Accuracy: 0.476562\n",
      "Epoch  1, Batch  82 -Loss:  6100.9399 Validation Accuracy: 0.496094\n",
      "Epoch  1, Batch  83 -Loss:  5104.7046 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  84 -Loss:  6279.7231 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  85 -Loss:  5097.5923 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  86 -Loss:  4625.0288 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  87 -Loss:  5527.1899 Validation Accuracy: 0.511719\n",
      "Epoch  1, Batch  88 -Loss:  5663.4619 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  89 -Loss:  6077.9531 Validation Accuracy: 0.503906\n",
      "Epoch  1, Batch  90 -Loss:  6144.4224 Validation Accuracy: 0.507812\n",
      "Epoch  1, Batch  91 -Loss:  5277.2725 Validation Accuracy: 0.515625\n",
      "Epoch  1, Batch  92 -Loss:  5173.0723 Validation Accuracy: 0.519531\n",
      "Epoch  1, Batch  93 -Loss:  5386.8594 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  94 -Loss:  5516.8911 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch  95 -Loss:  4781.9653 Validation Accuracy: 0.523438\n",
      "Epoch  1, Batch  96 -Loss:  4281.5078 Validation Accuracy: 0.535156\n",
      "Epoch  1, Batch  97 -Loss:  4898.6543 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  98 -Loss:  5449.0830 Validation Accuracy: 0.531250\n",
      "Epoch  1, Batch  99 -Loss:  4638.7734 Validation Accuracy: 0.527344\n",
      "Epoch  1, Batch 100 -Loss:  4520.9614 Validation Accuracy: 0.539062\n",
      "Epoch  1, Batch 101 -Loss:  5406.1758 Validation Accuracy: 0.542969\n",
      "Epoch  1, Batch 102 -Loss:  3994.0400 Validation Accuracy: 0.554688\n",
      "Epoch  1, Batch 103 -Loss:  6275.0547 Validation Accuracy: 0.558594\n",
      "Epoch  1, Batch 104 -Loss:  4455.7974 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 105 -Loss:  4898.5859 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 106 -Loss:  3910.4268 Validation Accuracy: 0.566406\n",
      "Epoch  1, Batch 107 -Loss:  4259.1494 Validation Accuracy: 0.562500\n",
      "Epoch  1, Batch 108 -Loss:  4901.7319 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 109 -Loss:  4452.4160 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 110 -Loss:  4808.4795 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 111 -Loss:  4581.9956 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 112 -Loss:  4787.6855 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 113 -Loss:  5427.2617 Validation Accuracy: 0.574219\n",
      "Epoch  1, Batch 114 -Loss:  4606.2158 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 115 -Loss:  4341.2061 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 116 -Loss:  3605.7280 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 117 -Loss:  4065.3604 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 118 -Loss:  4207.9883 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 119 -Loss:  3413.3369 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 120 -Loss:  4413.7573 Validation Accuracy: 0.578125\n",
      "Epoch  1, Batch 121 -Loss:  4483.6074 Validation Accuracy: 0.589844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 122 -Loss:  4240.2051 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 123 -Loss:  5047.9365 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 124 -Loss:  5244.3145 Validation Accuracy: 0.589844\n",
      "Epoch  1, Batch 125 -Loss:  3848.6658 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 126 -Loss:  4006.6699 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 127 -Loss:  3302.9810 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 128 -Loss:  4017.6785 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 129 -Loss:  4177.4214 Validation Accuracy: 0.593750\n",
      "Epoch  1, Batch 130 -Loss:  3613.5791 Validation Accuracy: 0.585938\n",
      "Epoch  1, Batch 131 -Loss:  3815.0581 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 132 -Loss:  4889.0479 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 133 -Loss:  4261.5752 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 134 -Loss:  2690.1741 Validation Accuracy: 0.601562\n",
      "Epoch  1, Batch 135 -Loss:  3902.7917 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 136 -Loss:  3979.0798 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 137 -Loss:  4179.1460 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 138 -Loss:  3703.4277 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 139 -Loss:  4169.0415 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 140 -Loss:  3448.8420 Validation Accuracy: 0.597656\n",
      "Epoch  1, Batch 141 -Loss:  3452.6440 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 142 -Loss:  4567.1562 Validation Accuracy: 0.609375\n",
      "Epoch  1, Batch 143 -Loss:  4074.8303 Validation Accuracy: 0.605469\n",
      "Epoch  1, Batch 144 -Loss:  3080.6753 Validation Accuracy: 0.613281\n",
      "Epoch  1, Batch 145 -Loss:  3699.5581 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 146 -Loss:  3045.3145 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 147 -Loss:  3288.7495 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 148 -Loss:  4417.0405 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 149 -Loss:  3247.8411 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 150 -Loss:  3397.9375 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 151 -Loss:  2827.7722 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 152 -Loss:  4517.5889 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 153 -Loss:  2917.0879 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 154 -Loss:  3208.6396 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 155 -Loss:  2864.5479 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 156 -Loss:  2782.1731 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 157 -Loss:  3667.7544 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 158 -Loss:  2160.2078 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 159 -Loss:  3856.5186 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 160 -Loss:  3513.3174 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 161 -Loss:  2866.1924 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 162 -Loss:  3640.3347 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 163 -Loss:  3397.2461 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 164 -Loss:  3464.4045 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 165 -Loss:  3413.0181 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 166 -Loss:  3219.2490 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 167 -Loss:  3329.9175 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 168 -Loss:  3422.9668 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 169 -Loss:  2841.3997 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 170 -Loss:  2695.9556 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 171 -Loss:  3628.5159 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 172 -Loss:  2934.6875 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 173 -Loss:  2997.1934 Validation Accuracy: 0.636719\n",
      "Epoch  1, Batch 174 -Loss:  3226.0959 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 175 -Loss:  3183.6182 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 176 -Loss:  2582.4302 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 177 -Loss:  2693.8008 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 178 -Loss:  3100.9021 Validation Accuracy: 0.621094\n",
      "Epoch  1, Batch 179 -Loss:  3259.7729 Validation Accuracy: 0.617188\n",
      "Epoch  1, Batch 180 -Loss:  2819.3821 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 181 -Loss:  2731.3308 Validation Accuracy: 0.632812\n",
      "Epoch  1, Batch 182 -Loss:  2336.4805 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 183 -Loss:  3087.0723 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 184 -Loss:  2830.0723 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 185 -Loss:  3705.7683 Validation Accuracy: 0.625000\n",
      "Epoch  1, Batch 186 -Loss:  2792.4021 Validation Accuracy: 0.628906\n",
      "Epoch  1, Batch 187 -Loss:  2367.2368 Validation Accuracy: 0.640625\n",
      "Epoch  1, Batch 188 -Loss:  2546.6099 Validation Accuracy: 0.644531\n",
      "Epoch  1, Batch 189 -Loss:  3393.4675 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 190 -Loss:  2816.6763 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 191 -Loss:  3053.1238 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 192 -Loss:  2674.2290 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 193 -Loss:  2616.7690 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 194 -Loss:  2627.0037 Validation Accuracy: 0.648438\n",
      "Epoch  1, Batch 195 -Loss:  2781.9226 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 196 -Loss:  2269.6768 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 197 -Loss:  3153.1030 Validation Accuracy: 0.652344\n",
      "Epoch  1, Batch 198 -Loss:  2181.1436 Validation Accuracy: 0.664062\n",
      "Epoch  1, Batch 199 -Loss:  2610.9817 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 200 -Loss:  3234.5312 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 201 -Loss:  3062.9785 Validation Accuracy: 0.656250\n",
      "Epoch  1, Batch 202 -Loss:  2434.1343 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 203 -Loss:  2778.0342 Validation Accuracy: 0.667969\n",
      "Epoch  1, Batch 204 -Loss:  3323.3901 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 205 -Loss:  2609.8069 Validation Accuracy: 0.660156\n",
      "Epoch  1, Batch 206 -Loss:  2706.8345 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 207 -Loss:  2850.0664 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 208 -Loss:  2728.0996 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 209 -Loss:  2445.6411 Validation Accuracy: 0.671875\n",
      "Epoch  1, Batch 210 -Loss:  2044.9253 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 211 -Loss:  2393.6162 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 212 -Loss:  2706.4604 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 213 -Loss:  2131.1699 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 214 -Loss:  2733.0693 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 215 -Loss:  2240.8899 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 216 -Loss:  2470.7148 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 217 -Loss:  2695.9277 Validation Accuracy: 0.675781\n",
      "Epoch  1, Batch 218 -Loss:  2746.5911 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 219 -Loss:  3089.9688 Validation Accuracy: 0.679688\n",
      "Epoch  1, Batch 220 -Loss:  2890.7188 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 221 -Loss:  2668.5786 Validation Accuracy: 0.683594\n",
      "Epoch  1, Batch 222 -Loss:  1880.0437 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 223 -Loss:  2696.7568 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 224 -Loss:  1913.6724 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 225 -Loss:  2043.0499 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 226 -Loss:  2564.6023 Validation Accuracy: 0.687500\n",
      "Epoch  1, Batch 227 -Loss:  2193.2866 Validation Accuracy: 0.691406\n",
      "Epoch  1, Batch 228 -Loss:  2351.1523 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 229 -Loss:  2368.7930 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 230 -Loss:  2245.4893 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 231 -Loss:  2613.8103 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 232 -Loss:  1872.6102 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 233 -Loss:  2579.4670 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 234 -Loss:  2133.0154 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 235 -Loss:  2146.6934 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 236 -Loss:  2311.3066 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 237 -Loss:  2805.7993 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 238 -Loss:  2533.2920 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 239 -Loss:  2261.9233 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 240 -Loss:  2049.0159 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 241 -Loss:  1772.0747 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 242 -Loss:  2074.0303 Validation Accuracy: 0.703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 243 -Loss:  2372.1895 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 244 -Loss:  2727.9194 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 245 -Loss:  1999.7917 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 246 -Loss:  1977.1771 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 247 -Loss:  2550.8525 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 248 -Loss:  2585.8225 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 249 -Loss:  2070.2676 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 250 -Loss:  1795.6704 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 251 -Loss:  3020.0059 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 252 -Loss:  2447.3381 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 253 -Loss:  2301.5361 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 254 -Loss:  2882.4595 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 255 -Loss:  1692.2283 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 256 -Loss:  2417.1924 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 257 -Loss:  2084.2434 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 258 -Loss:  1818.2352 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 259 -Loss:  1701.6470 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 260 -Loss:  2128.6340 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 261 -Loss:  2687.7285 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 262 -Loss:  2047.5593 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 263 -Loss:  3068.5757 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 264 -Loss:  2306.2566 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 265 -Loss:  2553.4194 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 266 -Loss:  1720.0090 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 267 -Loss:  2298.2502 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 268 -Loss:  2611.8608 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 269 -Loss:  2525.3391 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 270 -Loss:  2731.9568 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 271 -Loss:  2745.4788 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 272 -Loss:  1876.8750 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 273 -Loss:  1647.0286 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 274 -Loss:  2286.7222 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 275 -Loss:  2353.8999 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 276 -Loss:  1902.6750 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 277 -Loss:  2475.2300 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 278 -Loss:  2029.4036 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 279 -Loss:  2307.0952 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 280 -Loss:  2226.2720 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 281 -Loss:  1921.7812 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 282 -Loss:  2058.5498 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 283 -Loss:  1804.0276 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 284 -Loss:  1743.1108 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 285 -Loss:  1559.8676 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 286 -Loss:  2387.6606 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 287 -Loss:  2019.5597 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 288 -Loss:  1304.0964 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 289 -Loss:  1912.2424 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 290 -Loss:  2245.2515 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 291 -Loss:  1618.9880 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 292 -Loss:  1645.7560 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 293 -Loss:  2106.9805 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 294 -Loss:  2010.5758 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 295 -Loss:  1709.1616 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 296 -Loss:  3080.8496 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 297 -Loss:  2163.0149 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 298 -Loss:  2508.0435 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 299 -Loss:  2255.2854 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 300 -Loss:  1998.0148 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 301 -Loss:  2336.8271 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 302 -Loss:  1781.1855 Validation Accuracy: 0.695312\n",
      "Epoch  1, Batch 303 -Loss:  2423.0271 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 304 -Loss:  1944.6543 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 305 -Loss:  1654.7688 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 306 -Loss:  1666.3960 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 307 -Loss:  1372.5520 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 308 -Loss:  2206.0874 Validation Accuracy: 0.699219\n",
      "Epoch  1, Batch 309 -Loss:  1911.5856 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 310 -Loss:  1768.5046 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 311 -Loss:  1149.4847 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 312 -Loss:  2051.0642 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 313 -Loss:  2025.5156 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 314 -Loss:  2113.8003 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 315 -Loss:  2094.4590 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 316 -Loss:   938.0259 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 317 -Loss:  1943.1151 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 318 -Loss:  1327.7964 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 319 -Loss:  1782.8770 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 320 -Loss:  2278.1074 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 321 -Loss:  1338.9875 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 322 -Loss:  1909.0149 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 323 -Loss:  1971.6010 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 324 -Loss:  2371.8652 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 325 -Loss:  1945.2446 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 326 -Loss:  1815.8860 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 327 -Loss:  1893.0093 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 328 -Loss:  1764.2566 Validation Accuracy: 0.707031\n",
      "Epoch  1, Batch 329 -Loss:  1242.2769 Validation Accuracy: 0.703125\n",
      "Epoch  1, Batch 330 -Loss:  2369.6282 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 331 -Loss:  1472.7615 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 332 -Loss:  2078.3564 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 333 -Loss:  1679.9197 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 334 -Loss:  1994.3198 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 335 -Loss:  1617.1577 Validation Accuracy: 0.714844\n",
      "Epoch  1, Batch 336 -Loss:  1553.7419 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 337 -Loss:  1592.8177 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 338 -Loss:  1640.8535 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 339 -Loss:  1576.3602 Validation Accuracy: 0.710938\n",
      "Epoch  1, Batch 340 -Loss:  1470.3375 Validation Accuracy: 0.718750\n",
      "Epoch  1, Batch 341 -Loss:  2098.4546 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 342 -Loss:  1368.5459 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 343 -Loss:  1550.5388 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 344 -Loss:  2047.7759 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 345 -Loss:  2124.4927 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 346 -Loss:  1915.4028 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 347 -Loss:  1611.9463 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 348 -Loss:  1561.9001 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 349 -Loss:  1392.0977 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 350 -Loss:  1922.6721 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 351 -Loss:  1640.7671 Validation Accuracy: 0.722656\n",
      "Epoch  1, Batch 352 -Loss:  1589.6099 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 353 -Loss:  1292.7542 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 354 -Loss:  1952.3391 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 355 -Loss:  1970.5970 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 356 -Loss:  1715.6884 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 357 -Loss:  1527.1362 Validation Accuracy: 0.726562\n",
      "Epoch  1, Batch 358 -Loss:  1846.0306 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 359 -Loss:  2013.1816 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 360 -Loss:  1387.0568 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 361 -Loss:  1842.6794 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 362 -Loss:  1015.2576 Validation Accuracy: 0.726562\n",
      "Epoch  1, Batch 363 -Loss:  1763.2262 Validation Accuracy: 0.730469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 364 -Loss:  1824.2671 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 365 -Loss:  2036.9399 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 366 -Loss:  1830.2469 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 367 -Loss:  1253.8921 Validation Accuracy: 0.726562\n",
      "Epoch  1, Batch 368 -Loss:  1672.0436 Validation Accuracy: 0.726562\n",
      "Epoch  1, Batch 369 -Loss:  1603.3867 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 370 -Loss:  1172.4697 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 371 -Loss:  1196.0851 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 372 -Loss:  1749.0996 Validation Accuracy: 0.730469\n",
      "Epoch  1, Batch 373 -Loss:  1374.1348 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 374 -Loss:  1367.3420 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 375 -Loss:  1291.0751 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 376 -Loss:  1091.1830 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 377 -Loss:  1403.9434 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 378 -Loss:  1794.9016 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 379 -Loss:  1143.5729 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 380 -Loss:  1675.1697 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 381 -Loss:  2217.4497 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 382 -Loss:  1753.7252 Validation Accuracy: 0.734375\n",
      "Epoch  1, Batch 383 -Loss:  1809.2787 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 384 -Loss:  1673.0033 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 385 -Loss:  1266.1979 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 386 -Loss:   953.8417 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 387 -Loss:  1495.4055 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 388 -Loss:  1416.0211 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 389 -Loss:   968.2745 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 390 -Loss:  1481.6202 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 391 -Loss:  1401.6609 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 392 -Loss:  1625.7664 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 393 -Loss:  1091.8080 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 394 -Loss:  1694.9176 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 395 -Loss:   824.6270 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 396 -Loss:  1725.4136 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 397 -Loss:  1962.1968 Validation Accuracy: 0.742188\n",
      "Epoch  1, Batch 398 -Loss:  1416.3972 Validation Accuracy: 0.738281\n",
      "Epoch  1, Batch 399 -Loss:  1336.8271 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 400 -Loss:  1143.4539 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 401 -Loss:  1498.2051 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 402 -Loss:  1685.4873 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 403 -Loss:  1464.9175 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 404 -Loss:  1783.1589 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 405 -Loss:  1038.1625 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 406 -Loss:  1541.6824 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 407 -Loss:  1644.0571 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 408 -Loss:   786.4139 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 409 -Loss:  1317.2479 Validation Accuracy: 0.757812\n",
      "Epoch  1, Batch 410 -Loss:  1514.0378 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 411 -Loss:  1298.1484 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 412 -Loss:  1535.7090 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 413 -Loss:  1671.6479 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 414 -Loss:  1095.5796 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 415 -Loss:  1934.0098 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 416 -Loss:  1495.5381 Validation Accuracy: 0.757812\n",
      "Epoch  1, Batch 417 -Loss:  1964.0607 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 418 -Loss:  1494.5055 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 419 -Loss:  2156.8618 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 420 -Loss:  1252.0835 Validation Accuracy: 0.746094\n",
      "Epoch  1, Batch 421 -Loss:  1599.1246 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 422 -Loss:  1327.1628 Validation Accuracy: 0.757812\n",
      "Epoch  1, Batch 423 -Loss:  1243.7363 Validation Accuracy: 0.757812\n",
      "Epoch  1, Batch 424 -Loss:   946.2664 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 425 -Loss:  1301.7622 Validation Accuracy: 0.757812\n",
      "Epoch  1, Batch 426 -Loss:  1474.2169 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 427 -Loss:  1577.6340 Validation Accuracy: 0.750000\n",
      "Epoch  1, Batch 428 -Loss:  1581.1299 Validation Accuracy: 0.753906\n",
      "Epoch  1, Batch 429 -Loss:  1543.6635 Validation Accuracy: 0.746094\n",
      "Epoch  2, Batch   1 -Loss:  1126.1915 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch   2 -Loss:  1435.8529 Validation Accuracy: 0.746094\n",
      "Epoch  2, Batch   3 -Loss:  1217.5626 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch   4 -Loss:  1242.1895 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch   5 -Loss:  1343.6129 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch   6 -Loss:  1545.6460 Validation Accuracy: 0.753906\n",
      "Epoch  2, Batch   7 -Loss:  1487.1405 Validation Accuracy: 0.742188\n",
      "Epoch  2, Batch   8 -Loss:  1454.5437 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch   9 -Loss:  1297.2627 Validation Accuracy: 0.746094\n",
      "Epoch  2, Batch  10 -Loss:   936.0075 Validation Accuracy: 0.753906\n",
      "Epoch  2, Batch  11 -Loss:  1251.0027 Validation Accuracy: 0.753906\n",
      "Epoch  2, Batch  12 -Loss:  1458.5205 Validation Accuracy: 0.742188\n",
      "Epoch  2, Batch  13 -Loss:  1685.2080 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch  14 -Loss:  1474.4844 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch  15 -Loss:  1198.6533 Validation Accuracy: 0.746094\n",
      "Epoch  2, Batch  16 -Loss:  1530.4622 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch  17 -Loss:  1336.3345 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  18 -Loss:  1534.7397 Validation Accuracy: 0.753906\n",
      "Epoch  2, Batch  19 -Loss:   964.9112 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch  20 -Loss:  1291.1624 Validation Accuracy: 0.753906\n",
      "Epoch  2, Batch  21 -Loss:  1456.2236 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch  22 -Loss:  1376.8457 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  23 -Loss:  1057.9568 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  24 -Loss:  1351.9646 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  25 -Loss:  1114.3610 Validation Accuracy: 0.750000\n",
      "Epoch  2, Batch  26 -Loss:  1500.0183 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch  27 -Loss:  1393.5242 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  28 -Loss:  1471.1583 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  29 -Loss:   821.1797 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  30 -Loss:  1120.0577 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  31 -Loss:  1070.4497 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  32 -Loss:  1655.9904 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  33 -Loss:  1380.9460 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  34 -Loss:  1050.6996 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  35 -Loss:  1603.1836 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  36 -Loss:  1618.2092 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  37 -Loss:  1521.1050 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  38 -Loss:  1472.3330 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  39 -Loss:  1274.2047 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  40 -Loss:  1647.8961 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  41 -Loss:  1354.2723 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  42 -Loss:  1146.9191 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  43 -Loss:  1547.3049 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  44 -Loss:  1748.0760 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  45 -Loss:  1869.4573 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  46 -Loss:  1538.4556 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  47 -Loss:  1340.0471 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  48 -Loss:  1513.6875 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  49 -Loss:  1505.5408 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  50 -Loss:   926.0012 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  51 -Loss:  1017.7855 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  52 -Loss:  1219.8519 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  53 -Loss:  1176.8826 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  54 -Loss:  1142.1439 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  55 -Loss:  1581.7737 Validation Accuracy: 0.773438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Batch  56 -Loss:  1211.2814 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch  57 -Loss:  1884.3411 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch  58 -Loss:  1228.0466 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  59 -Loss:  1378.6270 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch  60 -Loss:  1201.5531 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  61 -Loss:  1530.5315 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  62 -Loss:  1032.3611 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  63 -Loss:   996.2045 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  64 -Loss:  1038.4033 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  65 -Loss:  1604.5879 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch  66 -Loss:  1554.7998 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch  67 -Loss:  1038.3491 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  68 -Loss:   852.0646 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  69 -Loss:   977.1086 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  70 -Loss:  1016.7549 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  71 -Loss:  1028.0942 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  72 -Loss:  1449.6835 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  73 -Loss:  1001.1085 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  74 -Loss:  1593.0433 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  75 -Loss:  1422.5852 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  76 -Loss:  1272.1156 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  77 -Loss:   989.0200 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  78 -Loss:  1113.6735 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  79 -Loss:   839.8840 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  80 -Loss:  1611.0760 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch  81 -Loss:  1092.5695 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  82 -Loss:  1298.2295 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  83 -Loss:  1337.3888 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  84 -Loss:  1132.2291 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  85 -Loss:  1200.3899 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  86 -Loss:  1362.6570 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  87 -Loss:  1575.1498 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  88 -Loss:  1617.2029 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  89 -Loss:  1378.3604 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  90 -Loss:  1237.5490 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  91 -Loss:  1201.2542 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  92 -Loss:  1229.2245 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch  93 -Loss:  1122.8175 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch  94 -Loss:  1221.5974 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  95 -Loss:   578.7046 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  96 -Loss:   752.0882 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  97 -Loss:  1203.3042 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch  98 -Loss:  1311.0978 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch  99 -Loss:  1601.8224 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch 100 -Loss:  1311.3085 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 101 -Loss:  1390.7487 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 102 -Loss:  1008.0482 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch 103 -Loss:  1285.3711 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 104 -Loss:  1071.9526 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch 105 -Loss:  1187.5812 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 106 -Loss:  1191.4824 Validation Accuracy: 0.757812\n",
      "Epoch  2, Batch 107 -Loss:  1832.8459 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch 108 -Loss:  1216.7177 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 109 -Loss:   953.8147 Validation Accuracy: 0.761719\n",
      "Epoch  2, Batch 110 -Loss:  1111.6160 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 111 -Loss:   927.7202 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 112 -Loss:  1134.5957 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 113 -Loss:   872.7825 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 114 -Loss:   721.3983 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 115 -Loss:   951.8989 Validation Accuracy: 0.765625\n",
      "Epoch  2, Batch 116 -Loss:  1064.0083 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 117 -Loss:  1593.0244 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 118 -Loss:  1015.4074 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 119 -Loss:  1361.8357 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 120 -Loss:  1142.5507 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 121 -Loss:   897.3232 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 122 -Loss:  1402.4038 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 123 -Loss:   940.8466 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 124 -Loss:  1295.7632 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 125 -Loss:  1194.7994 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 126 -Loss:   971.2224 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 127 -Loss:  1292.5616 Validation Accuracy: 0.769531\n",
      "Epoch  2, Batch 128 -Loss:  1393.0748 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 129 -Loss:  1588.5126 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 130 -Loss:   948.6702 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 131 -Loss:  1058.6368 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 132 -Loss:   735.7655 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 133 -Loss:  1045.2633 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 134 -Loss:  1426.0171 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 135 -Loss:  1241.5018 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 136 -Loss:   951.8496 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 137 -Loss:  1607.1696 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 138 -Loss:  1035.2651 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 139 -Loss:  1357.4077 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 140 -Loss:   919.2036 Validation Accuracy: 0.773438\n",
      "Epoch  2, Batch 141 -Loss:   633.8406 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 142 -Loss:  1037.5571 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 143 -Loss:   814.7755 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 144 -Loss:   944.0526 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 145 -Loss:  1099.5592 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 146 -Loss:   857.5816 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 147 -Loss:  1097.2849 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 148 -Loss:  1209.4783 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 149 -Loss:  1087.1211 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 150 -Loss:  1023.7924 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 151 -Loss:   737.6660 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 152 -Loss:  1592.4397 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 153 -Loss:  1472.0527 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 154 -Loss:  1072.8098 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 155 -Loss:  1031.1670 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 156 -Loss:   799.6395 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 157 -Loss:  1361.4729 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 158 -Loss:  1380.0635 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 159 -Loss:   900.9287 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 160 -Loss:  1044.4397 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 161 -Loss:  1048.1394 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 162 -Loss:  1248.5293 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 163 -Loss:  1289.9456 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 164 -Loss:   931.2954 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 165 -Loss:  1155.8088 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 166 -Loss:  1378.4028 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 167 -Loss:   792.2194 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 168 -Loss:  1236.3201 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 169 -Loss:  1137.9155 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 170 -Loss:  1058.6609 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 171 -Loss:   845.2719 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 172 -Loss:   388.9486 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 173 -Loss:  1267.1487 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 174 -Loss:  1029.5056 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 175 -Loss:   712.2816 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 176 -Loss:  1042.7523 Validation Accuracy: 0.785156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Batch 177 -Loss:   941.7053 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 178 -Loss:  1102.9440 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 179 -Loss:  1286.9812 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 180 -Loss:  1313.7683 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 181 -Loss:  1242.6495 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 182 -Loss:  1139.5754 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 183 -Loss:  1004.9172 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 184 -Loss:   836.3374 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 185 -Loss:   955.3136 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 186 -Loss:   700.2118 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 187 -Loss:  1173.9480 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 188 -Loss:   999.6101 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 189 -Loss:  1077.2919 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 190 -Loss:  1120.7515 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 191 -Loss:  1182.1285 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 192 -Loss:  1180.9398 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 193 -Loss:  1294.9703 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 194 -Loss:  1166.5494 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 195 -Loss:  1176.9454 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 196 -Loss:   891.6937 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 197 -Loss:   855.9680 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 198 -Loss:  1323.6399 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 199 -Loss:   908.9757 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 200 -Loss:   962.9112 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 201 -Loss:  1119.0121 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 202 -Loss:   978.3790 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 203 -Loss:   842.8329 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 204 -Loss:   730.1755 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 205 -Loss:  1012.8138 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 206 -Loss:  1193.1392 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 207 -Loss:  1271.1233 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 208 -Loss:  1014.6901 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 209 -Loss:  1076.7371 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 210 -Loss:   951.5223 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 211 -Loss:  1253.3490 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 212 -Loss:   797.6342 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 213 -Loss:  1186.3298 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 214 -Loss:  1295.9688 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 215 -Loss:   765.3762 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 216 -Loss:  1039.9817 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 217 -Loss:   868.0800 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 218 -Loss:  1072.4182 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 219 -Loss:  1045.8115 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 220 -Loss:  1013.3442 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 221 -Loss:   839.8292 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 222 -Loss:  1113.7524 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 223 -Loss:  1005.0563 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 224 -Loss:  1189.3931 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 225 -Loss:  1198.5613 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 226 -Loss:   929.1094 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 227 -Loss:  1605.5642 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 228 -Loss:   943.8152 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 229 -Loss:  1298.3892 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 230 -Loss:   867.3439 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 231 -Loss:  1116.5359 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 232 -Loss:  1142.5540 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 233 -Loss:   859.5458 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 234 -Loss:  1216.6270 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 235 -Loss:   682.7262 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 236 -Loss:   807.9236 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 237 -Loss:  1011.6764 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 238 -Loss:   956.2075 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 239 -Loss:   858.6556 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 240 -Loss:  1137.3381 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 241 -Loss:  1212.0894 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 242 -Loss:   988.3259 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 243 -Loss:  1266.6418 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 244 -Loss:  1569.2267 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 245 -Loss:  1185.1614 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 246 -Loss:  1009.3351 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 247 -Loss:  1166.9126 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 248 -Loss:   857.8809 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 249 -Loss:  1193.7316 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 250 -Loss:   942.3053 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 251 -Loss:   865.7788 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 252 -Loss:   878.6321 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 253 -Loss:   955.6376 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 254 -Loss:   983.4518 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 255 -Loss:  1219.8263 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 256 -Loss:   968.8115 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 257 -Loss:   739.1931 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 258 -Loss:   849.8261 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 259 -Loss:   805.0324 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 260 -Loss:   822.1137 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 261 -Loss:   814.0325 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 262 -Loss:  1001.0988 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 263 -Loss:   897.4447 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 264 -Loss:  1048.8245 Validation Accuracy: 0.777344\n",
      "Epoch  2, Batch 265 -Loss:   738.0289 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 266 -Loss:   844.4396 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 267 -Loss:  1598.1191 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 268 -Loss:  1060.1846 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 269 -Loss:  1229.8721 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 270 -Loss:   894.1718 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 271 -Loss:   791.6670 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 272 -Loss:  1240.3514 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 273 -Loss:   847.7881 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 274 -Loss:   888.9211 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 275 -Loss:   985.4409 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 276 -Loss:   615.5826 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 277 -Loss:   986.9991 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 278 -Loss:  1052.3385 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 279 -Loss:  1118.9424 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 280 -Loss:  1046.8081 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 281 -Loss:  1121.5817 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 282 -Loss:   796.2404 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 283 -Loss:   755.6067 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 284 -Loss:   758.2029 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 285 -Loss:  1214.5977 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 286 -Loss:  1026.9060 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 287 -Loss:   780.0737 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 288 -Loss:   965.3519 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 289 -Loss:   857.4045 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 290 -Loss:  1020.9232 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 291 -Loss:  1063.4836 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 292 -Loss:  1017.9882 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 293 -Loss:   836.8104 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 294 -Loss:  1171.1212 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 295 -Loss:   617.6883 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 296 -Loss:   895.4808 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 297 -Loss:   936.0931 Validation Accuracy: 0.800781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Batch 298 -Loss:  1029.2031 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 299 -Loss:   955.8584 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 300 -Loss:   566.7527 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 301 -Loss:  1094.0471 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 302 -Loss:  1306.4152 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 303 -Loss:   694.5383 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 304 -Loss:   782.4319 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 305 -Loss:   826.2412 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 306 -Loss:  1294.5156 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 307 -Loss:   887.7209 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 308 -Loss:   664.9961 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 309 -Loss:   814.9141 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 310 -Loss:   830.6384 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 311 -Loss:   729.8312 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 312 -Loss:   971.9158 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 313 -Loss:   768.5465 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 314 -Loss:   791.8967 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 315 -Loss:   775.6462 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 316 -Loss:   891.4932 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 317 -Loss:  1199.3848 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 318 -Loss:  1047.5684 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 319 -Loss:   633.3333 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 320 -Loss:  1261.7880 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 321 -Loss:   784.7782 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 322 -Loss:  1029.2705 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 323 -Loss:  1219.9921 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 324 -Loss:  1321.5100 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 325 -Loss:  1090.7446 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 326 -Loss:   958.3288 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 327 -Loss:   911.6444 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 328 -Loss:   727.8091 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 329 -Loss:   812.0710 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 330 -Loss:   733.5906 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 331 -Loss:   673.1478 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 332 -Loss:   987.7146 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 333 -Loss:  1265.4772 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 334 -Loss:   551.8458 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 335 -Loss:   896.7098 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 336 -Loss:   830.2468 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 337 -Loss:   859.2642 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 338 -Loss:   823.4849 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 339 -Loss:   879.0994 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 340 -Loss:   782.2020 Validation Accuracy: 0.781250\n",
      "Epoch  2, Batch 341 -Loss:   878.2426 Validation Accuracy: 0.785156\n",
      "Epoch  2, Batch 342 -Loss:  1058.1747 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 343 -Loss:  1160.3584 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 344 -Loss:   986.3962 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 345 -Loss:  1139.8936 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 346 -Loss:   844.4890 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 347 -Loss:   869.7579 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 348 -Loss:  1098.4537 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 349 -Loss:   891.5138 Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 350 -Loss:   679.3461 Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 351 -Loss:   746.1859 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 352 -Loss:   766.2496 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 353 -Loss:   815.9042 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 354 -Loss:   691.3361 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 355 -Loss:   951.1751 Validation Accuracy: 0.789062\n",
      "Epoch  2, Batch 356 -Loss:  1022.3154 Validation Accuracy: 0.792969\n",
      "Epoch  2, Batch 357 -Loss:   882.2764 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 358 -Loss:   427.1188 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 359 -Loss:   952.6561 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 360 -Loss:   784.7225 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 361 -Loss:   724.6501 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 362 -Loss:   816.4694 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 363 -Loss:   947.0020 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 364 -Loss:  1258.8538 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 365 -Loss:   668.4753 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 366 -Loss:   733.7307 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 367 -Loss:   663.2910 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 368 -Loss:   748.3024 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 369 -Loss:   743.2479 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 370 -Loss:   978.9648 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 371 -Loss:  1138.0514 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 372 -Loss:  1076.3943 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 373 -Loss:   897.1036 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 374 -Loss:   705.8170 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 375 -Loss:   924.8706 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 376 -Loss:   990.7239 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 377 -Loss:   948.0245 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 378 -Loss:   980.0283 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 379 -Loss:   826.7697 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 380 -Loss:   834.5382 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 381 -Loss:   879.1849 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 382 -Loss:   708.6156 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 383 -Loss:   479.2433 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 384 -Loss:   756.6006 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 385 -Loss:   791.3428 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 386 -Loss:   899.6077 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 387 -Loss:   655.3212 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 388 -Loss:   689.7410 Validation Accuracy: 0.812500\n",
      "Epoch  2, Batch 389 -Loss:  1145.9664 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 390 -Loss:   992.3761 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 391 -Loss:   805.2545 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 392 -Loss:   667.7775 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 393 -Loss:   717.7520 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 394 -Loss:   673.5043 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 395 -Loss:   689.4393 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 396 -Loss:   815.1647 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 397 -Loss:   815.1993 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 398 -Loss:  1043.1758 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 399 -Loss:  1174.2732 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 400 -Loss:   894.5435 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 401 -Loss:   890.8311 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 402 -Loss:   817.3044 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 403 -Loss:   858.3740 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 404 -Loss:   680.5749 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 405 -Loss:   934.5709 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 406 -Loss:   831.7489 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 407 -Loss:   696.0229 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 408 -Loss:   850.8900 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 409 -Loss:   663.0331 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 410 -Loss:   816.0249 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 411 -Loss:   632.9699 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 412 -Loss:   897.1510 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 413 -Loss:   800.3576 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 414 -Loss:   905.7167 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 415 -Loss:   821.5954 Validation Accuracy: 0.796875\n",
      "Epoch  2, Batch 416 -Loss:   675.3002 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 417 -Loss:   905.8774 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 418 -Loss:   830.4881 Validation Accuracy: 0.804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Batch 419 -Loss:   571.0338 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 420 -Loss:   700.7791 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 421 -Loss:   722.1876 Validation Accuracy: 0.800781\n",
      "Epoch  2, Batch 422 -Loss:  1080.1777 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 423 -Loss:   466.2278 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 424 -Loss:   684.7382 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 425 -Loss:   696.4144 Validation Accuracy: 0.808594\n",
      "Epoch  2, Batch 426 -Loss:   814.1727 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 427 -Loss:   530.2916 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 428 -Loss:   663.4431 Validation Accuracy: 0.804688\n",
      "Epoch  2, Batch 429 -Loss:   644.5878 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch   1 -Loss:   822.1523 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch   2 -Loss:   770.8873 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch   3 -Loss:   587.8857 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch   4 -Loss:   797.3102 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch   5 -Loss:   858.5239 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch   6 -Loss:   595.1307 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch   7 -Loss:   715.8553 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch   8 -Loss:   908.0026 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch   9 -Loss:   537.9872 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  10 -Loss:  1125.5352 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  11 -Loss:   440.4588 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  12 -Loss:   866.1849 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  13 -Loss:   654.8756 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  14 -Loss:   577.0033 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  15 -Loss:   785.5577 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  16 -Loss:   727.4638 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  17 -Loss:  1077.3121 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  18 -Loss:   687.1095 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  19 -Loss:   773.4728 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  20 -Loss:   912.6129 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  21 -Loss:   856.0265 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  22 -Loss:   999.8243 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  23 -Loss:   574.7058 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  24 -Loss:   832.3681 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  25 -Loss:   802.0963 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  26 -Loss:   524.4988 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  27 -Loss:   841.0781 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  28 -Loss:   764.9585 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  29 -Loss:   684.4169 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  30 -Loss:  1051.4434 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  31 -Loss:  1157.0254 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  32 -Loss:   702.7444 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  33 -Loss:   708.4955 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  34 -Loss:   638.7010 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  35 -Loss:   988.4208 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  36 -Loss:   507.5794 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  37 -Loss:  1059.4984 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  38 -Loss:   770.1057 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  39 -Loss:   470.8371 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  40 -Loss:   522.9715 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch  41 -Loss:   657.3585 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  42 -Loss:   697.6130 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  43 -Loss:   655.1418 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  44 -Loss:   686.6256 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  45 -Loss:   871.9683 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  46 -Loss:   660.9213 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  47 -Loss:   448.9318 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  48 -Loss:   567.8087 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  49 -Loss:   693.1364 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  50 -Loss:   765.2043 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  51 -Loss:   688.2159 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  52 -Loss:   557.9510 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  53 -Loss:   696.0686 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  54 -Loss:   993.5504 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  55 -Loss:   831.0623 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  56 -Loss:   503.2614 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  57 -Loss:   808.4450 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  58 -Loss:   558.6631 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  59 -Loss:   604.9496 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  60 -Loss:   730.2231 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  61 -Loss:   601.2158 Validation Accuracy: 0.796875\n",
      "Epoch  3, Batch  62 -Loss:   700.8485 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  63 -Loss:   603.2467 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  64 -Loss:   467.5148 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  65 -Loss:   784.3293 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  66 -Loss:   855.3347 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  67 -Loss:   572.7458 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  68 -Loss:   818.1698 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch  69 -Loss:   768.8637 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  70 -Loss:   551.0636 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  71 -Loss:   740.7395 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  72 -Loss:   751.8390 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  73 -Loss:   747.4254 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  74 -Loss:   914.5064 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  75 -Loss:   530.0397 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  76 -Loss:   934.3916 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  77 -Loss:   813.9616 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  78 -Loss:   777.0137 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  79 -Loss:   745.2336 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  80 -Loss:   433.9011 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  81 -Loss:   785.6362 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch  82 -Loss:  1183.3651 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  83 -Loss:   518.8167 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  84 -Loss:   473.1524 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  85 -Loss:   659.7894 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  86 -Loss:   431.3874 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  87 -Loss:   827.3577 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  88 -Loss:   791.7136 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  89 -Loss:   597.2678 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch  90 -Loss:   650.1539 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  91 -Loss:   827.6316 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  92 -Loss:   607.5540 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  93 -Loss:   511.6511 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  94 -Loss:   639.5598 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch  95 -Loss:  1115.8562 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  96 -Loss:   957.1564 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  97 -Loss:   204.1098 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  98 -Loss:   648.4922 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch  99 -Loss:   519.1685 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 100 -Loss:   640.6595 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 101 -Loss:   762.7589 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 102 -Loss:   609.3018 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 103 -Loss:   791.1612 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 104 -Loss:   684.5168 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 105 -Loss:   705.6700 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 106 -Loss:   808.3411 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 107 -Loss:  1138.3557 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 108 -Loss:   395.9656 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 109 -Loss:   565.6086 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 110 -Loss:   662.2605 Validation Accuracy: 0.808594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Batch 111 -Loss:   758.8974 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 112 -Loss:   767.1827 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 113 -Loss:   900.8086 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 114 -Loss:   606.7643 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 115 -Loss:   517.7330 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 116 -Loss:   742.7607 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 117 -Loss:   518.5599 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 118 -Loss:   521.7031 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 119 -Loss:  1047.4128 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 120 -Loss:   938.8848 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 121 -Loss:   635.7599 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 122 -Loss:   713.8346 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 123 -Loss:   376.8530 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 124 -Loss:   651.1869 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 125 -Loss:   515.9910 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 126 -Loss:   636.5125 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 127 -Loss:   768.4939 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 128 -Loss:   681.9135 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 129 -Loss:   623.2799 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 130 -Loss:   663.8091 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 131 -Loss:   865.3844 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 132 -Loss:  1076.5269 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 133 -Loss:   674.7177 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 134 -Loss:   436.4968 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 135 -Loss:   643.5857 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 136 -Loss:   487.4361 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 137 -Loss:   780.5557 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 138 -Loss:   709.4661 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 139 -Loss:  1075.5725 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 140 -Loss:   601.3643 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 141 -Loss:   736.0988 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 142 -Loss:   787.9659 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 143 -Loss:   694.9286 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 144 -Loss:   503.6051 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 145 -Loss:   927.8764 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 146 -Loss:   835.0449 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 147 -Loss:   576.7128 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 148 -Loss:   592.6528 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 149 -Loss:   671.4607 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 150 -Loss:  1036.0800 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 151 -Loss:   652.1025 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 152 -Loss:   705.3864 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 153 -Loss:   750.0554 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 154 -Loss:   930.0938 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 155 -Loss:   743.4846 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 156 -Loss:   814.7386 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 157 -Loss:   877.7570 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 158 -Loss:   666.4064 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 159 -Loss:   630.8284 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 160 -Loss:   604.7557 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 161 -Loss:   652.3671 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 162 -Loss:   930.7850 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 163 -Loss:   454.1201 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 164 -Loss:  1071.3055 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 165 -Loss:   449.9786 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 166 -Loss:   850.1047 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 167 -Loss:   727.9398 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 168 -Loss:   491.9770 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 169 -Loss:   753.6774 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 170 -Loss:   617.7629 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 171 -Loss:   937.1878 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 172 -Loss:   503.6580 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 173 -Loss:   452.4619 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 174 -Loss:   670.0353 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 175 -Loss:   698.3115 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 176 -Loss:   784.9523 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 177 -Loss:   638.8453 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 178 -Loss:   787.2262 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 179 -Loss:   719.3019 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 180 -Loss:   803.3025 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 181 -Loss:   464.5079 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 182 -Loss:   806.0206 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 183 -Loss:   798.2838 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 184 -Loss:   519.5862 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 185 -Loss:   541.0638 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 186 -Loss:   873.9316 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 187 -Loss:   756.5977 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 188 -Loss:   763.5251 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 189 -Loss:   883.5056 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 190 -Loss:   780.5364 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 191 -Loss:   728.6447 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 192 -Loss:   596.9736 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 193 -Loss:   694.1487 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 194 -Loss:   702.5345 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 195 -Loss:   425.1485 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 196 -Loss:   689.7230 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 197 -Loss:   728.9558 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 198 -Loss:   536.8625 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 199 -Loss:  1066.2529 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 200 -Loss:  1166.3818 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 201 -Loss:   633.3367 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 202 -Loss:   769.7684 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 203 -Loss:   514.7135 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 204 -Loss:   755.7007 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 205 -Loss:   518.1760 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 206 -Loss:  1033.7827 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 207 -Loss:   658.3783 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 208 -Loss:   827.8132 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 209 -Loss:   598.4238 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 210 -Loss:   800.9271 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 211 -Loss:   877.1914 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 212 -Loss:   470.5697 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 213 -Loss:   746.0980 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 214 -Loss:   580.7599 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 215 -Loss:   632.6232 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 216 -Loss:   483.2204 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 217 -Loss:   833.9285 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 218 -Loss:  1096.5724 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 219 -Loss:   735.0518 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 220 -Loss:   604.5302 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 221 -Loss:   945.3143 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 222 -Loss:   685.8824 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 223 -Loss:   643.4360 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 224 -Loss:   748.8301 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 225 -Loss:   775.5195 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 226 -Loss:   835.7661 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 227 -Loss:   658.8997 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 228 -Loss:   559.8194 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 229 -Loss:   593.6067 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 230 -Loss:   708.8754 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 231 -Loss:   577.9030 Validation Accuracy: 0.824219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Batch 232 -Loss:   752.4597 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 233 -Loss:   726.5206 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 234 -Loss:   627.4523 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 235 -Loss:   784.1642 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 236 -Loss:   749.8619 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 237 -Loss:   637.8702 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 238 -Loss:   530.7336 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 239 -Loss:  1007.4362 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 240 -Loss:   508.8937 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 241 -Loss:   715.1449 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 242 -Loss:   543.4341 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 243 -Loss:   536.5328 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 244 -Loss:   448.3575 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 245 -Loss:   810.3583 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 246 -Loss:   651.3232 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 247 -Loss:   744.2872 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 248 -Loss:   521.2058 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 249 -Loss:   478.7134 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 250 -Loss:   672.2972 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 251 -Loss:   632.7915 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 252 -Loss:   614.4767 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 253 -Loss:   490.0420 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 254 -Loss:   596.7461 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 255 -Loss:   863.6142 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 256 -Loss:   656.3278 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 257 -Loss:   669.1409 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 258 -Loss:   607.2897 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 259 -Loss:   652.6820 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 260 -Loss:   739.9451 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 261 -Loss:   778.9304 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 262 -Loss:   559.1053 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 263 -Loss:   708.4855 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 264 -Loss:   725.8303 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 265 -Loss:   620.6307 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 266 -Loss:   714.3423 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 267 -Loss:   824.2926 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 268 -Loss:   559.3942 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 269 -Loss:   787.4220 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 270 -Loss:   611.2778 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 271 -Loss:   554.8576 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 272 -Loss:   514.6676 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 273 -Loss:   966.7870 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 274 -Loss:   615.1475 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 275 -Loss:   672.9359 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 276 -Loss:   607.0298 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 277 -Loss:   529.1783 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 278 -Loss:   579.3889 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 279 -Loss:   594.7314 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 280 -Loss:   525.2664 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 281 -Loss:   582.6767 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 282 -Loss:   558.1248 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 283 -Loss:   809.2430 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 284 -Loss:   854.0132 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 285 -Loss:   837.6697 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 286 -Loss:   445.9669 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 287 -Loss:   514.6755 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 288 -Loss:   703.0792 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 289 -Loss:   712.9343 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 290 -Loss:   650.4475 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 291 -Loss:   719.8817 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 292 -Loss:  1005.9308 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 293 -Loss:   559.5745 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 294 -Loss:   711.6368 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 295 -Loss:   496.4208 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 296 -Loss:   756.5576 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 297 -Loss:   823.3689 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 298 -Loss:   519.5253 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 299 -Loss:   739.2585 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 300 -Loss:   515.8759 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 301 -Loss:   486.6448 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 302 -Loss:   421.3639 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 303 -Loss:   623.7230 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 304 -Loss:   548.9742 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 305 -Loss:   602.9033 Validation Accuracy: 0.832031\n",
      "Epoch  3, Batch 306 -Loss:   529.5884 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 307 -Loss:   497.1343 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 308 -Loss:   711.0188 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 309 -Loss:   770.9873 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 310 -Loss:   636.6042 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 311 -Loss:   686.8923 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 312 -Loss:   464.6726 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 313 -Loss:   472.6455 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 314 -Loss:   599.2235 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 315 -Loss:   577.9313 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 316 -Loss:   761.6320 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 317 -Loss:   635.4703 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 318 -Loss:   779.3832 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 319 -Loss:   793.3363 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 320 -Loss:   375.5292 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 321 -Loss:   353.3737 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 322 -Loss:   546.8774 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 323 -Loss:   368.4199 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 324 -Loss:   509.7603 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 325 -Loss:   739.2788 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 326 -Loss:   609.2378 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 327 -Loss:   674.4059 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 328 -Loss:  1014.8187 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 329 -Loss:   410.2412 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 330 -Loss:   766.8823 Validation Accuracy: 0.835938\n",
      "Epoch  3, Batch 331 -Loss:   684.4647 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 332 -Loss:   417.8558 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 333 -Loss:   833.3413 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 334 -Loss:   556.6478 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 335 -Loss:   538.3417 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 336 -Loss:   477.2584 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 337 -Loss:   660.3983 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 338 -Loss:   558.8540 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 339 -Loss:   380.1520 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 340 -Loss:   641.3647 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 341 -Loss:   590.2857 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 342 -Loss:   870.0272 Validation Accuracy: 0.828125\n",
      "Epoch  3, Batch 343 -Loss:   812.6278 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 344 -Loss:   354.4169 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 345 -Loss:   427.6664 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 346 -Loss:   744.4496 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 347 -Loss:   487.7104 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 348 -Loss:   738.5557 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 349 -Loss:   583.0975 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 350 -Loss:   612.8654 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 351 -Loss:   709.9181 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 352 -Loss:   629.2260 Validation Accuracy: 0.816406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Batch 353 -Loss:   595.5592 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 354 -Loss:   615.7335 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 355 -Loss:   515.9161 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 356 -Loss:   794.3850 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 357 -Loss:   669.3032 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 358 -Loss:   630.5645 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 359 -Loss:   872.9854 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 360 -Loss:   641.0637 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 361 -Loss:   569.0335 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 362 -Loss:   919.0256 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 363 -Loss:  1107.1853 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 364 -Loss:   775.1362 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 365 -Loss:   770.4422 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 366 -Loss:   461.9546 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 367 -Loss:   481.0488 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 368 -Loss:   417.4565 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 369 -Loss:   478.9452 Validation Accuracy: 0.824219\n",
      "Epoch  3, Batch 370 -Loss:   518.1167 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 371 -Loss:   536.5093 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 372 -Loss:   625.9312 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 373 -Loss:   744.2917 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 374 -Loss:   637.4017 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 375 -Loss:   678.8930 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 376 -Loss:   578.6201 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 377 -Loss:   642.9963 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 378 -Loss:   647.4171 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 379 -Loss:   678.4941 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 380 -Loss:   722.8267 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 381 -Loss:   605.4272 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 382 -Loss:   734.9098 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 383 -Loss:   740.7736 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 384 -Loss:   597.7894 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 385 -Loss:   561.8075 Validation Accuracy: 0.820312\n",
      "Epoch  3, Batch 386 -Loss:   756.0469 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 387 -Loss:   705.9934 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 388 -Loss:   516.6701 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 389 -Loss:   704.5304 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 390 -Loss:   262.6492 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 391 -Loss:   863.2831 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 392 -Loss:   606.2194 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 393 -Loss:   505.9640 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 394 -Loss:   442.1384 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 395 -Loss:   503.1533 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 396 -Loss:   602.4601 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 397 -Loss:   302.1004 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 398 -Loss:   490.7293 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 399 -Loss:   616.7387 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 400 -Loss:   640.6102 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 401 -Loss:   683.5638 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 402 -Loss:   863.2906 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 403 -Loss:   736.5557 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 404 -Loss:   743.5359 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 405 -Loss:   417.7774 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 406 -Loss:   339.2945 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 407 -Loss:   457.6589 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 408 -Loss:   481.6006 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 409 -Loss:   679.5777 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 410 -Loss:   772.0438 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 411 -Loss:   483.6025 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 412 -Loss:   619.7566 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 413 -Loss:   695.3682 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 414 -Loss:   618.2911 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 415 -Loss:   628.3553 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 416 -Loss:   751.0246 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 417 -Loss:   443.4320 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 418 -Loss:   880.0021 Validation Accuracy: 0.816406\n",
      "Epoch  3, Batch 419 -Loss:   613.1022 Validation Accuracy: 0.800781\n",
      "Epoch  3, Batch 420 -Loss:   393.9615 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 421 -Loss:   590.9708 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 422 -Loss:   861.1992 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 423 -Loss:   421.9163 Validation Accuracy: 0.812500\n",
      "Epoch  3, Batch 424 -Loss:   439.5229 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 425 -Loss:   516.9403 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 426 -Loss:   602.2958 Validation Accuracy: 0.804688\n",
      "Epoch  3, Batch 427 -Loss:   521.1733 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 428 -Loss:   420.4876 Validation Accuracy: 0.808594\n",
      "Epoch  3, Batch 429 -Loss:   529.7819 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch   1 -Loss:   661.3021 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch   2 -Loss:   908.7305 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch   3 -Loss:   571.0867 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch   4 -Loss:   808.3278 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch   5 -Loss:   722.6537 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch   6 -Loss:   768.6316 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch   7 -Loss:   552.2786 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch   8 -Loss:   372.0466 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch   9 -Loss:   527.5447 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  10 -Loss:   520.8450 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  11 -Loss:   695.0276 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  12 -Loss:   392.9070 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  13 -Loss:   785.2893 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  14 -Loss:   706.1901 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  15 -Loss:   708.0406 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  16 -Loss:   598.9463 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  17 -Loss:   472.5170 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  18 -Loss:   435.7164 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  19 -Loss:   640.8220 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch  20 -Loss:   478.9532 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  21 -Loss:   750.8737 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  22 -Loss:   703.6277 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch  23 -Loss:   380.3545 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  24 -Loss:   555.4010 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  25 -Loss:   525.8306 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  26 -Loss:   784.3687 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  27 -Loss:   541.0203 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  28 -Loss:   371.2571 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  29 -Loss:   726.2900 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  30 -Loss:   690.2438 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch  31 -Loss:   404.7210 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch  32 -Loss:   782.2230 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  33 -Loss:   289.7250 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  34 -Loss:   529.3148 Validation Accuracy: 0.804688\n",
      "Epoch  4, Batch  35 -Loss:   638.9727 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  36 -Loss:   484.4243 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  37 -Loss:   627.3513 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  38 -Loss:   330.9068 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  39 -Loss:   720.8718 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  40 -Loss:   725.4265 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  41 -Loss:   541.2224 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  42 -Loss:   513.9152 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  43 -Loss:   432.8889 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  44 -Loss:   530.5834 Validation Accuracy: 0.824219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4, Batch  45 -Loss:   640.0205 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  46 -Loss:   490.3087 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  47 -Loss:   443.8665 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  48 -Loss:   869.0760 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  49 -Loss:   546.7388 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  50 -Loss:   577.8777 Validation Accuracy: 0.832031\n",
      "Epoch  4, Batch  51 -Loss:   323.1937 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  52 -Loss:   548.0154 Validation Accuracy: 0.832031\n",
      "Epoch  4, Batch  53 -Loss:   737.1351 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  54 -Loss:   464.8737 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  55 -Loss:   595.2281 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  56 -Loss:   417.4953 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  57 -Loss:   453.6413 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  58 -Loss:   937.9980 Validation Accuracy: 0.828125\n",
      "Epoch  4, Batch  59 -Loss:   435.6619 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  60 -Loss:   565.7172 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  61 -Loss:   514.6146 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  62 -Loss:   588.8685 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  63 -Loss:   569.7637 Validation Accuracy: 0.812500\n",
      "Epoch  4, Batch  64 -Loss:   616.0829 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  65 -Loss:   498.5069 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  66 -Loss:   588.3936 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  67 -Loss:   506.0957 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  68 -Loss:   349.0307 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  69 -Loss:   714.8849 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  70 -Loss:   594.8491 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  71 -Loss:   487.2076 Validation Accuracy: 0.808594\n",
      "Epoch  4, Batch  72 -Loss:   399.9550 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  73 -Loss:   494.3917 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  74 -Loss:   861.3123 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  75 -Loss:   391.3488 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  76 -Loss:   342.5765 Validation Accuracy: 0.824219\n",
      "Epoch  4, Batch  77 -Loss:   323.0937 Validation Accuracy: 0.816406\n",
      "Epoch  4, Batch  78 -Loss:   640.0999 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  79 -Loss:   290.8431 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  80 -Loss:   689.0487 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  81 -Loss:   493.5822 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  82 -Loss:   714.1692 Validation Accuracy: 0.820312\n",
      "Epoch  4, Batch  83 -Loss:   636.7152 Validation Accuracy: 0.816406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a0a709d70f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_valid_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_valid_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 keep_prob: 1.})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             print('Epoch {:>2}, Batch {:>3} -'\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf. global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(mnist.train.num_examples//batch_size):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: dropout})\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_x,\n",
    "                y: batch_y,\n",
    "                keep_prob: 1.})\n",
    "            valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: mnist.validation.images[:test_valid_size],\n",
    "                y: mnist.validation.labels[:test_valid_size],\n",
    "                keep_prob: 1.})\n",
    "\n",
    "            print('Epoch {:>2}, Batch {:>3} -'\n",
    "                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                epoch + 1,\n",
    "                batch + 1,\n",
    "                loss,\n",
    "                valid_acc))\n",
    "\n",
    "    # Calculate Test Accuracy\n",
    "    test_acc = sess.run(accuracy, feed_dict={\n",
    "        x: mnist.test.images[:test_valid_size],\n",
    "        y: mnist.test.labels[:test_valid_size],\n",
    "        keep_prob: 1.})\n",
    "    print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! That is a CNN in TensorFlow. Now that you've seen a CNN in TensorFlow, let's see if you can apply it on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. TensorFlow Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolution Layers in TensorFlow\n",
    "Let's now apply what we've learned to build real CNNs in TensorFlow. In the below exercise, you'll be asked to set up the dimensions of the Convolution filters, the weights, the biases. This is in many ways the trickiest part to using CNNs in TensorFlow. Once you have a sense of how to set up the dimensions of these attributes, applying CNNs will be far more straight forward.\n",
    "\n",
    "## Review\n",
    "You should go over the TensorFlow documentation for [2D convolutions](https://www.tensorflow.org/api_guides/python/nn#Convolution). Most of the documentation is straightforward, except perhaps the padding argument. The padding might differ depending on whether you pass 'VALID' or 'SAME'.\n",
    "\n",
    "Here are a few more things worth reviewing:\n",
    "\n",
    "1. Introduction to TensorFlow -> TensorFlow Variables.\n",
    "2. How to determine the dimensions of the output based on the input size and the filter size (shown below). You'll use this to determine what the size of your filter should be.\n",
    "```\n",
    " new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    " new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n",
    "\n",
    "## Instructions\n",
    "1. Finish off each TODO in the conv2d function.\n",
    "2. Setup the strides, padding and filter weight/bias (F_w and F_b) such that the output shape is (1, 2, 2, 3). Note that all of these except strides should be TensorFlow variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup the strides, padding and filter weight/bias such that\n",
    "the output shape is (1, 2, 2, 3).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    # TODO: Define the filter weights `F_W` and filter bias `F_b`.\n",
    "    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.\n",
    "    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [1, 2, 2, 1]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = 'VALID'\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d\n",
    "    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.\n",
    "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "\n",
    "out = conv2d(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. Solution: TensorFlow Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "Here's how I did it. NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine.\n",
    "\n",
    "```python\n",
    "def conv2d(input):\n",
    "    # Filter (weights and bias)\n",
    "    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))\n",
    "    F_b = tf.Variable(tf.zeros(3))\n",
    "    strides = [1, 2, 2, 1]\n",
    "    padding = 'VALID'\n",
    "    return tf.nn.conv2d(input, F_W, strides, padding) + F_b\n",
    "```\n",
    "\n",
    "I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 3). I choose 'VALID' for the padding algorithm. I find it simpler to understand and it achieves the result I'm looking for.\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "\n",
    "In order to change the depth from 1 to 3, I have to set the output depth of my filter appropriately:\n",
    "\n",
    "```python\n",
    "F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3))) # (height, width, input_depth, output_depth)\n",
    "F_b = tf.Variable(tf.zeros(3)) # (output_depth)\n",
    "```\n",
    "\n",
    "The input has a depth of 1, so I set that as the input_depth of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33. TensorFlow Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pooling Layers in TensorFlow\n",
    "In the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for [tf.nn.max_pool()](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool). Padding works the same as it does for a convolution.\n",
    "\n",
    "## Instructions\n",
    "1. Finish off each TODO in the maxpool function.\n",
    "2. Setup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the values to `strides` and `ksize` such that\n",
    "the output shape after pooling is (1, 2, 2, 1).\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)\n",
    "# (1, 4, 4, 1)\n",
    "x = np.array([\n",
    "    [0, 1, 0.5, 10],\n",
    "    [2, 2.5, 1, -8],\n",
    "    [4, 0, 5, 6],\n",
    "    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))\n",
    "X = tf.constant(x)\n",
    "\n",
    "def maxpool(input):\n",
    "    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [?, ?, ?, ?]\n",
    "    # TODO: Set the stride for each dimension (batch_size, height, width, depth)\n",
    "    strides = [?, ?, ?, ?]\n",
    "    # TODO: set the padding, either 'VALID' or 'SAME'.\n",
    "    padding = ?\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "    \n",
    "out = maxpool(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 34. Solution: TensorFlow Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Here's how I did it. NOTE: there's more than 1 way to get the correct output shape. Your answer might differ from mine.\n",
    "\n",
    "```python\n",
    "def maxpool(input):\n",
    "    ksize = [1, 2, 2, 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    padding = 'VALID'\n",
    "    return tf.nn.max_pool(input, ksize, strides, padding)\n",
    "```\n",
    "\n",
    "I want to transform the input shape (1, 4, 4, 1) to (1, 2, 2, 1). I choose 'VALID' for the padding algorithm. I find it simpler to understand and it achieves the result I'm looking for.\n",
    "\n",
    "```\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "Plugging in the values:\n",
    "\n",
    "```\n",
    "out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2\n",
    "```\n",
    "The depth doesn't change during a pooling operation so I don't have to worry about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35. Lab: LeNet In TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/screenshot-2016-11-26-17.52.14.png\">\n",
    "<center>LeNet. Source: Yann Lecun.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to put together everything you've learned and implement the [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) architecture using TensorFlow.\n",
    "\n",
    "When you get to your next project, remember that LeNet can be a great starting point for your network architecture!\n",
    "\n",
    "## Instructions:\n",
    "1. Set up your development environment with the [CarND Starter Kit](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/83ec35ee-1e02-48a5-bdb7-d244bd47c2dc/lessons/8c82408b-a217-4d09-b81d-1bda4c6380ef/concepts/4f1870e0-3849-43e4-b670-12e6f2d4b7a7)\n",
    "2. git clone https://github.com/udacity/CarND-LeNet-Lab.git\n",
    "3. cd CarND-LeNet-Lab\n",
    "4. jupyter notebook\n",
    "5. Finish off the architecture implementation in the LeNet function. That's the only piece that's missing.\n",
    "\n",
    "## Preprocessing\n",
    "An MNIST image is initially 784 features (1D). If the data is not normalized from [0, 255] to [0, 1], normalize it. We reshape this to (28, 28, 1) (3D), and pad the image with 0s such that the height and width are 32 (centers digit further). Thus, the input shape going into the first convolutional layer is 32x32x1.\n",
    "\n",
    "## Specs\n",
    "**Convolution layer 1.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation 1.** Your choice of activation function.\n",
    "\n",
    "**Pooling layer 1.** The output shape should be 14x14x6.\n",
    "\n",
    "**Convolution layer 2.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation 2.** Your choice of activation function.\n",
    "\n",
    "**Pooling layer 2.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten layer.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.\n",
    "\n",
    "**Fully connected layer 1.** This should have 120 outputs.\n",
    "\n",
    "**Activation 3.** Your choice of activation function.\n",
    "\n",
    "**Fully connected layer 2.** This should have 84 outputs.\n",
    "\n",
    "**Activation 4.** Your choice of activation function.\n",
    "\n",
    "**Fully connected layer 3.** This should have 10 outputs.\n",
    "\n",
    "You'll return the result of the final fully connected layer from the LeNet function.\n",
    "\n",
    "If implemented correctly you should see output similar to the following:\n",
    "\n",
    "```\n",
    "EPOCH 1 ...\n",
    "Validation loss = 52.809\n",
    "Validation accuracy = 0.864\n",
    "\n",
    "EPOCH 2 ...\n",
    "Validation loss = 24.749\n",
    "Validation accuracy = 0.915\n",
    "\n",
    "EPOCH 3 ...\n",
    "Validation loss = 17.719\n",
    "Validation accuracy = 0.930\n",
    "\n",
    "EPOCH 4 ...\n",
    "Validation loss = 12.188\n",
    "Validation accuracy = 0.943\n",
    "\n",
    "EPOCH 5 ...\n",
    "Validation loss = 8.935\n",
    "Validation accuracy = 0.954\n",
    "\n",
    "EPOCH 6 ...\n",
    "Validation loss = 7.674\n",
    "Validation accuracy = 0.956\n",
    "\n",
    "EPOCH 7 ...\n",
    "Validation loss = 6.822\n",
    "Validation accuracy = 0.956\n",
    "\n",
    "EPOCH 8 ...\n",
    "Validation loss = 5.451\n",
    "Validation accuracy = 0.961\n",
    "\n",
    "EPOCH 9 ...\n",
    "Validation loss = 4.881\n",
    "Validation accuracy = 0.964\n",
    "\n",
    "EPOCH 10 ...\n",
    "Validation loss = 4.623\n",
    "Validation accuracy = 0.964\n",
    "\n",
    "Test loss = 4.726\n",
    "Test accuracy = 0.962\n",
    "```\n",
    "\n",
    "## Parameters Galore\n",
    "As an additional fun exercise calculate the total number of parameters used by the network. Note, the convolutional layers use weight sharing!\n",
    "\n",
    "Supporting Materials\n",
    "[lenet.py](http://video.udacity-data.com.s3.amazonaws.com/topher/2016/December/5850423d_lenet/lenet.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 36. Solution: LeNet In TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "You can see [one implementation of the solution](https://github.com/udacity/CarND-LeNet-Lab/blob/master/LeNet-Lab-Solution.ipynb) in the GitHub repo.\n",
    "\n",
    "Here is the LeNet function:\n",
    "\n",
    "```python\n",
    "def LeNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
    "    fc0   = flatten(conv2)\n",
    "\n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(10))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "    return logits\n",
    "```\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "Let's go through this solution layer by layer.\n",
    "```python\n",
    "# Hyperparameters\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "```\n",
    "\n",
    "This solution uses the tf.truncated_normal() function to initialize the weights and bias Variables. Using the default mean and standard deviation from tf.truncated_normal() is fine. However, tuning these hyperparameters can result in better performance.\n",
    "\n",
    "```python\n",
    "# SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "conv1_b = tf.Variable(tf.zeros(6))\n",
    "conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "```\n",
    "\n",
    "This layer transforms the Tensor 32x32x1 to 28x28x6.\n",
    "\n",
    "Use a filter with the shape (5, 5, 1, 6) with VALID padding.\n",
    "\n",
    "Recall the shape has dimensions: (height, width, input_depth, output_depth).\n",
    "\n",
    "With VALID padding, the formula for the new height and width is:\n",
    "\n",
    "```\n",
    "out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
    "```\n",
    "\n",
    "Plugging in values:\n",
    "\n",
    "```python\n",
    "out_height = ceil(float(32 - 5 + 1) / float(1)) = 28\n",
    "out_width = ceil(float(32 - 5 + 1) / float(1)) = 28\n",
    "```\n",
    "\n",
    "Clearly, the strides must equal 1, or the output would be too small.\n",
    "\n",
    "```python\n",
    "# SOLUTION: Activation.\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "```\n",
    "\n",
    "A standard ReLU activation. You might have chosen another activation.\n",
    "\n",
    "```python\n",
    "# SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "```\n",
    "\n",
    "The formula to calculate the new height and width for a pooling layer mirrors the formula for a convolutional layer.\n",
    "\n",
    "```python\n",
    "new_height = ceil(float(28 - 2 + 1) / float(2)) = ceil(13.5) = 14\n",
    "new_width = ceil(float(28 - 2 + 1) / float(2)) = ceil(13.5) = 14\n",
    "```\n",
    "\n",
    "The next round of convolution -> activation -> pooling uses an identical methodology.\n",
    "\n",
    "```python\n",
    "# SOLUTION: Flatten Layer.\n",
    "fc0 = flatten(conv2)\n",
    "```\n",
    "\n",
    "The flatten function flattens a Tensor into two dimensions: (batches, length). The batch size remains unaltered, so all of the other dimensions of the input Tensor are flattened into the second dimension of the output Tensor.\n",
    "\n",
    "In this model, the the output shape of Pooling Layer 2 should be 5x5x16 (ignoring batch size). Applying flatten will multiply the length of each dimension together, which equals 400.\n",
    "\n",
    "Now that the Tensor is 2D, it's ready to be used in fully connected layers.\n",
    "\n",
    "```python\n",
    "# SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "fc1_b = tf.Variable(tf.zeros(120))\n",
    "fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "# SOLUTION: Activation.\n",
    "fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "# SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "fc2_b  = tf.Variable(tf.zeros(84))\n",
    "fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "# SOLUTION: Activation.\n",
    "fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "# SOLUTION: Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "fc3_b  = tf.Variable(tf.zeros(10))\n",
    "logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "```\n",
    "\n",
    "You're already familiar with fully connected layers so I won't go into much detail. Note the output sizes: 120, 84, and 10.\n",
    "\n",
    "Congratulations! You're now a convolution and pooling expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 37. CNNs - Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "There are many wonderful free resources that allow you to go into more depth around Convolutional Neural Networks. In this course, our goal is to give you just enough intuition to start applying this concept on real world problems so you have enough of an exposure to explore more on your own. We strongly encourage you to explore some of these resources more to reinforce your intuition and explore different ideas.\n",
    "\n",
    "These are the resources we recommend in particular:\n",
    "\n",
    "- Andrej Karpathy's [CS231n Stanford course](http://cs231n.github.io/) on Convolutional Neural Networks.\n",
    "- Michael Nielsen's [free book](http://neuralnetworksanddeeplearning.com/) on Deep Learning.\n",
    "- Goodfellow, Bengio, and Courville's more advanced [free book](http://deeplearningbook.org/) on Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
