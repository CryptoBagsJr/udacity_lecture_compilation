{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding\n",
    "\n",
    "## By Seonman Kim\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Summary\n",
    "\n",
    "I created 'Advance Line Finding-Final.ipynb' that includes all the functions and codes used to execute the lane finding algorithm and to create a video as an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The camera calibration is as follows:\n",
    "\n",
    "* Load all the images used for calibration. ('./camera_cal/calibration\\*.jpg')\n",
    "* For each image:\n",
    "    * convert the image to GRAY color\n",
    "        * GRAY = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    * Find the chessboard corners\n",
    "        * ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "        * We save corner points (corners) for CameraCalibration later.\n",
    "    * If ret == True, draw and display the corners\n",
    "        * cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "* Do camera calibration given object points and image points\n",
    "    * ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)\n",
    "* We save mtx (calibration matrix) and dist (distortion coefficients) for later use.\n",
    "\n",
    "\n",
    "The following is a sample output of Camera calibration:\n",
    "\n",
    "<img src='./output_images/CameraCalibration.png'>\n",
    "\n",
    "## Undistort images\n",
    "\n",
    "Using the camera calibration matrix and distortion coefficient obtained from above, we could undistort images using:\n",
    "\n",
    "```\n",
    "dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "```\n",
    "\n",
    "The followings are the sample outputs:\n",
    "\n",
    "<img src='./output_images/Undistortion.png'>\n",
    "\n",
    "## Warp images using perspective transformation\n",
    "\n",
    "Warping the images is done by selecting a set of source and destination points, and then applying the points to `getPerspectiveTransform()` and `warpPerspective()` functions.\n",
    "\n",
    "```\n",
    "    # Source and destination points\n",
    "    src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "    dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                            [img_size[0]-offset, img_size[1]-offset], \n",
    "                            [offset, img_size[1]-offset]])\n",
    "    \n",
    "    # Given src and dst points, calculate the perspective transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Warp the image using OpenCV warpPerspective()\n",
    "    warped = cv2.warpPerspective(img, M, img_size)\n",
    "```\n",
    "\n",
    "Here is a sample output of warping with perspective transformation.\n",
    "\n",
    "<img src='./output_images/UndistortedAndWarped.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (single images)\n",
    "\n",
    "### 1. Distortion Correction\n",
    "\n",
    "The first step in the processing pipeline was to undistort the input image using the camera calibration matrix and distortion coefficients that we obtained from the previous Camera Calibration step. \n",
    "\n",
    "<img src='./output_images/Pipeline-undistorted.png'>\n",
    "\n",
    "### 2. Perspective Transformation\n",
    "I found that applying the perspective transform first gave better results and made it easier to see what the polynomial fits were trying to fit to. \n",
    "\n",
    "As described above, we have used `getPerspectiveTransform()` and `warpPerspective()` functions to perform perspective transformation. The following source and destination points were used:\n",
    "\n",
    "```python\n",
    "    src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "    dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                            [img_size[0]-offset, img_size[1]-offset], \n",
    "                            [offset, img_size[1]-offset]])\n",
    "```\n",
    "\n",
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 580, 460      | 200, 100        | \n",
    "| 200, 720      | 200, 720      |\n",
    "| 706, 460     | 1040, 100      |\n",
    "| 1140, 720      | 1040, 720        |\n",
    "\n",
    "I verified that the perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image.\n",
    "\n",
    "<img src='./output_images/Pipeline-BirdEyeView.png'>\n",
    "\n",
    "### 3. Gradients and color transformations for thresholded binary image\n",
    "\n",
    "I used several different transformations to create the final binary_warped image.\n",
    "\n",
    "I used python library `ipywidgets` and `interact_manual` functions to interactively test multiple threshold values and find the paramters for the best output.\n",
    "\n",
    "I found the following transformation and threshold made the best result.\n",
    "\n",
    "#### Absolute Sobel X \n",
    "\n",
    "* Min threshold: 20\n",
    "* Max threshold: 120\n",
    "\n",
    "<img src='./output_images/Pipeline-Abs-Sobel.png'>\n",
    "\n",
    "#### Magnitude of the gradient\n",
    "\n",
    "* Kernel size: 15\n",
    "* Min threshold: 10\n",
    "* Max threshold: 150\n",
    "\n",
    "<img src='./output_images/Pipeline-Magnitude.png'>\n",
    "\n",
    "####  Sobel Direction \n",
    "\n",
    "* Kernel size: 5\n",
    "* Min threshold: 0.15\n",
    "* Max threshold: 0.60\n",
    "\n",
    "<img src='./output_images/Pipeline-Direction.png'>\n",
    "\n",
    "#### Color HLS S Channel\n",
    "\n",
    "* Min threshold: 170\n",
    "* Max threshold: 220\n",
    "\n",
    "<img src='./output_images/Pipeline-HLS-S.png'>\n",
    "\n",
    "#### Combining the transforms\n",
    "\n",
    "We didn't use abs_sobel_thresh on y axis, because it didn't help much.\n",
    "\n",
    "We combined four transformations to create a binary_warped image, as follows:\n",
    "\n",
    "```\n",
    "    # Perspective Transform\n",
    "    img_unwarp, M, Minv = unwarp(img_undistort, src, dst)\n",
    "\n",
    "    gradx = abs_sobel_thresh(img_unwarp, orient='x', sobel_kernel=3, thresh=(20, 120))\n",
    "    # grady = abs_sobel_thresh(bird, orient='y', sobel_kernel=ksize, thresh=(20, 100))\n",
    "    mag_binary = mag_thresh(img_unwarp, sobel_kernel=15, mag_thresh=(10, 150))\n",
    "    dir_binary = dir_threshold(img_unwarp, sobel_kernel=5, thresh=(0.16, 0.61))\n",
    "\n",
    "    # HLS S-Channel\n",
    "    hls_s_binary = hls_sthresh(img_unwarp, (170, 220))\n",
    "\n",
    "    binary_warped = np.zeros_like(dir_binary)\n",
    "    # combined[((gradx == 1) & (grady == 1)) | ((mag_binary == 1) & (dir_binary == 1))] = 1\n",
    "    binary_warped[((gradx == 1)) | ((mag_binary == 1) & (dir_binary == 1)) | (hls_s_binary == 1)] = 1\n",
    "\n",
    "```\n",
    "\n",
    "### Sample outputs\n",
    "\n",
    "<img src='./output_images/Pipeline-Output1.png'>\n",
    "<img src='./output_images/Pipeline-Output2.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Lane Line Curve Fitting\n",
    "\n",
    "After creating the binary_warped image, the next step was to apply a histogram to the image to determine the peak locations of pixels in the image. Once the histogram was found, a sliding window technique was used to isolate the lane line pixels in the image. \n",
    "\n",
    "### Histogram and Sliding Windows\n",
    "\n",
    "This was a critical step in the code for determining the proper left and right lane lines. The sliding windows (rectangles) start with their center at the X locations were the histogram peaks occur. The sliding windows are then extended 120 pixels in the +/- X directions (margin = 120) and look for a minimum of 60 pixels. If this condition is met, then the next sliding window will be cetered at the average X location of these pixels, otherwise they will continue to center around the histogram peaks. Once the lane pixel indices and X,Y locations are determined, a 2nd order polynomial fit was used to fit lines to these pixels for both the left and right lane lines using the numpy polyfit function.\n",
    "\n",
    "```\n",
    "left_fit = np.polyfit(lefty, leftx, 2)\n",
    "right_fit = np.polyfit(righty, rightx, 2)\n",
    "```\n",
    "\n",
    "An example of the histogram and the sliding window fit is shown below.\n",
    "\n",
    "<img src='./output_images/Pipeline-test5.png'>\n",
    "\n",
    "<img src='./output_images/Pipeline-Histogram.png'>\n",
    "\n",
    "<img src='./output_images/Pipeline-SlidingWindow.png'>\n",
    "\n",
    "\n",
    "Still within the `sliding_windows()` function, the next steps are where I spent the most time and achieved the biggest gains in results. I used several different techniques to check that the determined polynomial fits for each lane line were realistic.\n",
    "\n",
    "1. Limits to Polynomial Coefficients\n",
    "    - I created several made up curves that were similar in curvature to what the lane lines would be. Using this information I limited the values that the squared and linear terms could take.\n",
    "2. Confidence of left/right lanes\n",
    "    - Based on the number of pixels found corresponding to each lane line, I implemented a “confidence” metric. To determine the optimal number of pixels found for the left and right lanes, I compared the number of pixels found during the relatively easy straight lines images with images that had high curvature, shadows and other road markings.\n",
    "3. Lane Width Check\n",
    "    - The distance lane width should not change drastically, it should constantly be around 800 pixels in my implementation. I applied a check for this and if the lane lines were too far apart or too close together, I would discard the information from the lane lines for that timestep if the condition was not met.\n",
    "4. Averaging the Fits\n",
    "    - I stored the fits from each images an applied an averaging (smoothing) over the most recent 20 images. Since the video is taken at 25 fps, this corresponds to smoothing of approximately 1 second of images. This helped to stablize the line fits and reduce the jitter that could be seen in the video.\n",
    "\n",
    "\n",
    "### Find Lines\n",
    "\n",
    "`find_lines()` used the curve fits from the previous timestep to determine the new lines. Once the sliding window techqiue is performed in the first frame, the base of the left/right lanes nearest to the vehicle should not change greatly from frame to frame. Thus, the previous step's curve fit along with a margin was used to determine the current timestep fits. An example of this technique is shown below with the search area around the polynomial curves highlighted in green:\n",
    "\n",
    "\n",
    "<img src='./output_images/Pipeline-FindLines.png'>\n",
    "\n",
    "### Visualizing the lanes and calculating lane information\n",
    "\n",
    "A function called rad_curve() draws the lane lines on the image, determines the radius of curvature of the lines and determines the car’s position relative to the center of the lanes. The polynomial fits have the form:\n",
    "\n",
    "$f(y) = Ay^2 + By + C$\n",
    "\n",
    "From this equation, the radius of curvature can then be calculated at any point x of the function x=f(y) is given as follows:\n",
    "\n",
    "$R_{curve} = \\frac{[1 + (\\frac{dx}{dy})^2]^{3/2}}{|\\frac{d^2x}{dy^2}|}$\n",
    "\n",
    "The first and second derivatives of the second order polynomial used to fit the lane lines are:\n",
    "\n",
    "$f'(y) = \\frac{dx}{dy} = 2Ay + B$\n",
    "\n",
    "$f''(y) = \\frac{d^2x}{dy^2} = 2A$\n",
    "\n",
    "\n",
    "So, the equation for radius of curvature becomes:\n",
    "\n",
    "$R_{curve} = \\frac{(1 + (2Ay + B)^2)3/2}{|2A|}$\n",
    "\n",
    "The following figure show the output.\n",
    "\n",
    "<img src='./output_images/Pipeline-LineArea.png'>\n",
    "\n",
    "## Sample output\n",
    "\n",
    "<img src='./output_images/Sample-outputs.png'>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Output (video)\n",
    "\n",
    "Here's a [link to my video result in Youtube.](https://youtu.be/fjo6dsGXLVk)\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "#### Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "This project took relatively a large amount of time when it compares to other (deep learning) projects. The first camera calibration, undistorion and unwarpiong images were relative easy. But when it comes to color and sobel gradients transformation, I had to spend much time to find good threshold parameters. Luckily, I found ipywidgets library, and could do it interactively by manually setting the parameters. It was very helpful.\n",
    "\n",
    "And, finding lines with sliding windows and curve coefficients were a little difficult to understand. I tried to refer the source code in the lecture, and it took much times.\n",
    "\n",
    "Luckily, the output of the process pipeline was not bad, and I am glad on the final result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
